[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AWS Cloud Practitioner Notes",
    "section": "",
    "text": "Preface\nWelcome to your comprehensive online resource for preparing for the AWS Certified Cloud Practitioner exam. This course is meticulously designed for individuals aiming to acquire a fundamental understanding of the Amazon Web Services (AWS) Cloud, transcending specific technical roles. It is an ideal starting point for gaining insights into the essentials of AWS Cloud, including its services, security, architecture, pricing, and support frameworks.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-aims",
    "href": "index.html#course-aims",
    "title": "AWS Cloud Practitioner Notes",
    "section": "Course Aims",
    "text": "Course Aims\nThrough this course, participants will:\n\nGrasp a foundational understanding of AWS and its operational definitions.\nCompare and contrast various cloud deployment models such as on-premises, hybrid-cloud, and exclusively cloud-based solutions.\nLearn about the AWS global infrastructure and the pivotal role of Availability Zones.\nUnderstand the six key advantages of leveraging the AWS Cloud.\nGain knowledge about primary AWS services across computing, networking, databases, and storage.\nEvaluate appropriate AWS solutions for diverse use cases.\nExplore the AWS Well-Architected Framework and the shared responsibility model.\nDive into the fundamental aspects of AWS security services and cloud migration strategies.\nDiscuss the financial implications of using AWS in terms of cost management and billing.\nUtilize AWS pricing tools to make informed, cost-effective decisions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "AWS Cloud Practitioner Notes",
    "section": "Course outline",
    "text": "Course outline\n\nModule 1: Introduction to Amazon Web Services\n\nSummarize the benefits of AWS\nDescribe differences between on-demand delivery and cloud deployments\nSummarize the pay-as-you-go pricing model\n\n\n\nModule 2: Compute in the Cloud\n\nDescribe the benefits of Amazon Elastic Compute Cloud (Amazon EC2) at a basic level\nIdentify the different Amazon EC2 instance types\nDifferentiate between the various billing options for Amazon EC2\nDescribe the benefits of Amazon EC2 Auto Scaling\nSummarize the benefits of Elastic Load Balancing\nGive an example of the uses for Elastic Load Balancing\nSummarize the differences between Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Services (Amazon SQS)\nSummarize additional AWS compute options\n\n\n\nModule 3: Global Infrastructure and Reliability\n\nSummarize the benefits of the AWS Global Infrastructure\nDescribe the basic concept of Availability Zones\nDescribe the benefits of Amazon CloudFront and Edge locations\nCompare different methods for provisioning AWS services\n\n\n\nModule 4: Networking\n\nDescribe the basic concepts of networking\nDescribe the difference between public and private networking resources\nExplain a virtual private gateway using a real life scenario\nExplain a virtual private network (VPN) using a real life scenarioDescribe the benefit of AWS Direct Connect\nDescribe the benefit of hybrid deployments\nDescribe the layers of security used in an IT strategy\nDescribe which services are used to interact with the AWS global network\n\n\n\nModule 5: Storage and Databases\n\nSummarize the basic concept of storage and databases\nDescribe benefits of Amazon Elastic Block Store (Amazon EBS)\nDescribe benefits of Amazon Simple Storage Service (Amazon S3)\nDescribe the benefits of Amazon Elastic File System (Amazon EFS)\nSummarize various storage solutions\nDescribe the benefits of Amazon Relational Database Service (Amazon RDS)\nDescribe the benefits of Amazon DynamoDB\nSummarize various database services\n\n\n\nModule 6: Security\n\nExplain the benefits of the shared responsibility model\nDescribe multi-factor authentication (MFA)\nDifferentiate between the AWS Identity and Access Management (IAM) security levels\nDescribe security policies at a basic level\nExplain the benefits of AWS Organizations\nSummarize the benefits of compliance with AWS\nExplain primary AWS security services at a basic level\n\n\n\nModule 7: Monitoring and Analytics\n\nSummarize approaches to monitoring your AWS environment\nDescribe the benefits of Amazon CloudWatch\nDescribe the benefits of AWS CloudTrail\nDescribe the benefits of AWS Trusted Advisor\n\n\n\nModule 8: Pricing and Support\n\nUnderstand AWS pricing and support models\nDescribe the AWS Free Tier\nDescribe key benefits of AWS Organizations and consolidated billing\nExplain the benefits of AWS Budgets\nExplain the benefits of AWS Cost Explorer\nExplain the primary benefits of the AWS Pricing Calculator\nDistinguish between the various AWS Support Plans\nDescribe the benefits of AWS Marketplace\n\n\n\nModule 9: Migration and Innovation\n\nUnderstand migration and innovation in the AWS Cloud\nSummarize the AWS Cloud Adoption Framework (AWS CAF)\nSummarize six key factors of a cloud migration strategy\nDescribe the benefits of various AWS data migration solutions, such as AWS Snowcone, AWS Snowball, and AWS Snowmobile\nSummarize the broad scope of innovative solutions that AWS offers\n\n\n\nModule 10: The Cloud Journey\n\nSummarize the six pillars of the AWS Well-Architected Framework\nExplain the six benefits of cloud computing\n\n\n\nModule 11: AWS Certified Cloud Practitioner Basics\n\nDetermine resources for preparing for the AWS Certified Cloud Practitioner examination\nDescribe benefits of becoming AWS Certified\n\nEach module is followed by quizzes to test your knowledge and reinforce learning. Ensure you engage with these quizzes and review the material as needed to solidify your understanding.\nThis course serves as a pathway to not only prepare you for the AWS Certified Cloud Practitioner exam but also to instil a robust foundation in AWS Cloud, empowering you to navigate and utilize the cloud more effectively in your professional journey.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "module-1-notes/Cloud-Computing.html",
    "href": "module-1-notes/Cloud-Computing.html",
    "title": "1  Cloud Computing",
    "section": "",
    "text": "1.1 Deployment models for cloud computing\nWhen choosing a cloud strategy, a company needs to evaluate several aspects including the necessary components for cloud applications, the preferred tools for managing resources, and the requirements of any existing legacy IT infrastructure.\nThere are three main models for deploying cloud computing: cloud-based, on-premises, and hybrid.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "module-1-notes/Cloud-Computing.html#cloud-based",
    "href": "module-1-notes/Cloud-Computing.html#cloud-based",
    "title": "1  Cloud Computing",
    "section": "1.2 Cloud-based",
    "text": "1.2 Cloud-based\n\nOperate all application components within the cloud environment.\nTransfert existing applications to the cloud setting.\nDevelop and construct new applications directly in the cloud.\n\nIn a cloud-based deployment model, it’s feasible to transition existing applications into the cloud, or alternatively, to design and initiate new applications within the cloud framework. These applications can be developed atop low-level infrastructure, which necessitates management by your IT team. Alternatively, you could utilise more advanced services that diminish the need for extensive management, architecture, and scaling of the underlying infrastructure.\nIn a cloud-based deployment model, it is possible to migrate current applications to the cloud or to develop and launch new applications within the cloud framework. These apps can be built on low-level infrastructure, requiring management by your IT personnel. Alternatively, you might use more advanced services that require less comprehensive management, architecture, and scaling of the underlying infrastructure.\nFor instance, a business might develop an application that includes virtual servers, databases, and networking components, all operating entirely within the cloud.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "module-1-notes/Cloud-Computing.html#on-premises",
    "href": "module-1-notes/Cloud-Computing.html#on-premises",
    "title": "1  Cloud Computing",
    "section": "1.3 On-premises",
    "text": "1.3 On-premises\nDeploy resources using virtualisation and resource management tools. Boost resource use by employing application management and virtualisation technologies. On-premises deployment, also known as private cloud deployment, involves setting up resources on-site with these tools.\nFor example, you might have applications running on technology housed entirely in your on-premises data centre. While this setup is similar to traditional IT setups, using application management and virtualisation technologies helps make better use of resources.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "module-1-notes/Cloud-Computing.html#hybrid",
    "href": "module-1-notes/Cloud-Computing.html#hybrid",
    "title": "1  Cloud Computing",
    "section": "1.4 Hybrid",
    "text": "1.4 Hybrid\nn a hybrid deployment, you connect cloud resources to your on-premises setup. This method is useful in several scenarios. For instance, you might have older applications that are best kept on-site, or there may be regulations that require your company to store certain data on-site.\nConsider a company that wants to use cloud services for automating batch data processing and analytics, but has several older applications that are more suitable for on-premises use and won’t be moved to the cloud. With a hybrid deployment, this company could maintain these legacy applications on-site while still taking advantage of cloud-based data and analytics services.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Cloud Computing</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-Instance-Types.html",
    "href": "module-2-notes/Amazon-EC2-Instance-Types.html",
    "title": "2  EC2 Instance types",
    "section": "",
    "text": "Amazon EC2 instance types are optimised for various tasks. Your workloads and applications’ unique requirements should guide your choice of instance type. This may include requirements for computation, memory, and storage capacities.\nThese instances can be summarised into the following five categories:.\n\n2.0.1 1. General Purpose Instances\nGeneral purpose instances offer a balanced set of compute, memory, and networking resources. These are suitable for applications that have moderate load requirements with occasional spikes in usage.\nIn other words, general purpose instances are like Swiss Army knives in the world of cloud computing. They are versatile and can handle a variety of tasks well. They provide a balanced mix of resources: compute (processing power for calculations), memory (storage for data), and networking (data transfer capabilities).\nYou may use them for a range of workloads, including :\n\nApplication servers: These are the engines that run your applications. For example, if you have a web application like an online store, the application server is what processes the customer’s requests, like browsing items, adding them to the cart, and checking out.\nGaming servers: These servers host multiplayer online games. For instance, if you’re playing an online game like Fortnite or Minecraft with friends, the gaming server is what keeps track of everyone’s actions and game progress.\nBackend servers for enterprise applications: These servers handle the behind-the-scenes operations of large business applications. For example, in a banking system, the backend server would handle tasks like processing transactions, updating account balances, and generating financial reports.\nSmall and medium databases: These are the digital equivalent of record-keeping systems. They store, retrieve, and organize data. For instance, a small business might have a database that keeps track of inventory, sales, and customer information.\n\n\n\n2.0.2 2. Compute optimized Instances\nCompute-optimised instances are ideal for compute-bound applications that benefit from high-performance processors. They are like the sprinters in the world of cloud computing. They are designed for tasks that require a lot of processing power, just like a sprinter is built for speed.\nHere are some examples:\n\nHigh-performance web servers: These are like the super-fast express trains on the internet. They handle a lot of web traffic and need to respond to user requests quickly. For example, a popular e-commerce site during a big sale event would need a high-performance web server to handle the surge in traffic.\nCompute-intensive application servers: These are like the heavy-duty machines in a factory, crunching through complex calculations or processing large amounts of data. For instance, a weather forecasting system that needs to process data from thousands of weather stations and run complex simulations would be a compute-intensive application.\nDedicated gaming servers: These are like the high-speed racetracks for online multiplayer games, where fast response times are crucial for a good gaming experience. For example, a fast-paced online multiplayer game like Call of Duty would benefit from a dedicated gaming server.\nBatch processing workloads: These are like the assembly lines in a factory, processing many transactions in a single group. For example, a billing system that generates invoices for thousands of customers at the end of the month would use batch processing.\n\n\n\n2.0.3 3. Memory Optimized Instances\nMemory optimized instances are designed to provide rapid performance for tasks that handle extensive datasets in memory. In computing, memory serves as a provisional storage space. It accommodates all the data and directives required by a Central Processing Unit (CPU) to execute operations. Prior to the operation of a computer program or application, it is transferred from storage to memory. This act of preloading allows the CPU to have immediate access to the computer program.\nIn other words, memory optimized instances are like the big, spacious warehouses of cloud computing. They are designed to handle tasks that require a lot of space for storing data temporarily. This is similar to a warehouse where goods are stored before they are shipped out.\nMemory optimized instances can be used for:\n\nHigh-performance databases: These are like huge libraries with millions of books (data). The books need to be readily available (loaded into memory) for quick access. For example, a global e-commerce site like Amazon would use a high-performance database to store and quickly retrieve product information, customer data, and transaction details.\nReal-time processing of large unstructured data: This is like sorting through a big pile of mixed items (unstructured data) in real-time. For instance, a social media platform like X (Twitter) might use this to analyze and categorize millions of tweets as they are posted.\n\n\n\n2.0.4 4. Accelerated Computing Instances\nAccelerated computing instances employ hardware accelerators, also known as coprocessors, to carry out certain tasks more effectively than software running on CPUs can. In computing, a hardware accelerator is a device that can speed up data processing. Accelerated computing instances are perfectly suited for workloads like graphics applications, streaming games, and streaming applications.\nAccelerated computing instances can be used for :\n\nFloating-point number calculations: These are complex mathematical calculations that involve numbers with decimals. For example, scientific simulations or financial modeling applications that need to perform a lot of these calculations would benefit from accelerated computing.\nGraphics processing: This is like rendering a high-definition video or creating a 3D animation. For instance, a graphic design software or a video game would need to process a lot of graphics data quickly.\nData pattern matching: This is like finding a specific pattern in a large dataset. For example, a search engine looking for specific keywords in a large database of web pages would use data pattern matching.\n\n\n\n2.0.5 5. Storage Optimized Instances\nStorage optimized instances are designed for tasks that require high, sequential read and write access to extensive datasets on local storage. In other words, storage optimized instances are like the big cargo trucks of cloud computing. They are designed to handle tasks that involve a lot of loading and unloading of data, similar to a cargo truck transporting goods.\nIn computing, the term input/output operations per second (IOPS) is a performance metric for a storage device. It indicates the number of distinct input or output operations a device can execute in a single second. Storage optimized instances are engineered to deliver tens of thousands of low-latency, random IOPS to applications.\nInput operations can be thought of as data introduced into a system, like records entered into a database. An output operation is data produced by a server. An example of output could be the analytics conducted on the records in a database. If you have an application with a high IOPS demand, a storage optimized instance can offer superior performance compared to other instance types that are not optimized for this specific use case.\nStorage optimized instances can be used for :\n\nDistributed file systems: These are like a network of warehouses (storage spaces), where data is stored across multiple locations for easy access. For example, a cloud storage service like Dropbox or Google Drive would use a distributed file system.\nData warehousing applications: These are like huge data libraries that store, organize, and analyze large amounts of data. For instance, a business might use a data warehouse to analyze customer behavior, sales trends, and market research.\nHigh-frequency online transaction processing (OLTP) systems: These are like busy supermarkets where lots of transactions (like buying and selling of goods) are happening all at once. For example, an e-commerce site like Amazon during a big sale event would have a high-frequency OLTP system.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>EC2 Instance types</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-pricing.html",
    "href": "module-2-notes/Amazon-EC2-pricing.html",
    "title": "3  Amazon EC2 pricing",
    "section": "",
    "text": "3.1 1. On-Demand Instances\nOn-Demand Instances allow you to pay for compute capacity by the hour or second (minimum of 60 seconds) with no long-term commitments or upfront payments. This option is ideal for applications with short-term, irregular workloads that cannot be interrupted. For example, they are perfect for developing and testing applications where you don’t know the exact workload in advance.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**Amazon EC2 pricing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-pricing.html#reserved-instances",
    "href": "module-2-notes/Amazon-EC2-pricing.html#reserved-instances",
    "title": "3  Amazon EC2 pricing",
    "section": "3.2 2. Reserved Instances",
    "text": "3.2 2. Reserved Instances\nReserved Instances provide you with the option to reserve EC2 computing capacity for 1 or 3 years, in exchange for a significantly discounted hourly rate (up to 75% compared to On-Demand pricing). This is suitable for applications with steady state or predictable usage and provides budget predictability. A common use case is for databases or enterprise applications where steady usage patterns are anticipated.\nThere are two available types of Reserved Instances:\n\nStandard Reserved Instances\nConvertible Reserved Instances\n\nStandard Reserved and Convertible Reserved Instances are available for purchase on a 1-year or 3-year basis. The 3-year option provides larger expense savings.\nStandard Reserved Instances: This choice is appropriate if you are aware of the EC2 instance type and size required for your steady-state applications, as well as the AWS region in which they will be deployed. Reserved instances require that you state the following qualifications:\n\nInstance type and size: For example, m5.xlarge\nPlatform description (operating system): For example, Microsoft Windows Server or Red Hat Enterprise Linux\nTenancy: Default tenancy or dedicated tenancy\n\nYou can choose an availability zone for your EC2 reserved instances. If you make this specification, you will receive an EC2 capacity reservation. This ensures that your chosen number of EC2 instances are available when you need them.\nConvertible Reserved Instances: These can be the best option for you if you need to run your EC2 instances across different availability zones or instance types. Note: If you need to be more flexible with how you run your EC2 instances, you have to give up a greater discount.\nAt the end of a Reserved Instance period, you can continue to use the Amazon EC2 instance without interruption. However, you are charged on-demand rates until you do any of the following:\n\nTerminate the instance.\nPurchase a new reserved instance that matches the instance attributes (instance family and size, region, platform, and tenancy).",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**Amazon EC2 pricing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-pricing.html#ec2-instance-savings-plans",
    "href": "module-2-notes/Amazon-EC2-pricing.html#ec2-instance-savings-plans",
    "title": "3  Amazon EC2 pricing",
    "section": "3.3 3. EC2 Instance Savings Plans",
    "text": "3.3 3. EC2 Instance Savings Plans\nThese plans provide a cost-saving strategy for Amazon EC2, in which you commit to a set hourly expenditure on EC2 instances within a specific instance family and region for a one- or three-year period. This allows you to save up to 72% compared to the on-demand pricing approach. The agreed-upon pricing applies to consumption up to your commitment level (e.g., $10 per hour), with any further usage charged at standard on-demand rates.\nFlexibility is one of the primary benefits of EC2 Instance Savings Plans. You benefit from decreased costs when using any EC2 instance from the selected instance family in the defined region, regardless of availability zone, instance size, operating system, or tenancy. This flexibility differs from Standard Reserved Instances, which need specified upfront commitments for instance type, size, and other parameters but do not require a fixed number of instances or an EC2 capacity reserve.\nAWS also provides tools such as AWS Cost Explorer to help you manage and optimise your expenditures. This tool helps in analysing your usage and expenditures over time and offers tailored recommendations for savings plans based on your past EC2 usage.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**Amazon EC2 pricing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-pricing.html#spot-instances",
    "href": "module-2-notes/Amazon-EC2-pricing.html#spot-instances",
    "title": "3  Amazon EC2 pricing",
    "section": "3.4 4. Spot Instances",
    "text": "3.4 4. Spot Instances\nSpot Instances let you take advantage of unused EC2 capacity in the AWS cloud at steep discounts relative to On-Demand prices, up to 90% off. Spot Instances are perfect for workloads that are flexible in when they can run and can handle interruptions, such as batch data processing, background processing, or optional tasks.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**Amazon EC2 pricing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Amazon-EC2-pricing.html#dedicated-hosts",
    "href": "module-2-notes/Amazon-EC2-pricing.html#dedicated-hosts",
    "title": "3  Amazon EC2 pricing",
    "section": "3.5 5. Dedicated Hosts",
    "text": "3.5 5. Dedicated Hosts\nDedicated Hosts are physical servers with EC2 instance capacity fully dedicated to your use. They help you address compliance requirements and reduce costs by allowing you to use your existing server-bound software licenses. These are particularly useful for regulatory requirements that may not support multi-tenant virtualization or for running software that has strict licensing terms that require dedicated physical servers.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>**Amazon EC2 pricing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Scaling-Amazon-EC2.html",
    "href": "module-2-notes/Scaling-Amazon-EC2.html",
    "title": "4  Scaling Amazon EC2",
    "section": "",
    "text": "Scalability is the process of starting with just the resources you need and designing your architecture to automatically adjust to changing demand by scaling out (adding resources) or in (reducing resources). This ensures you pay only for the resources you actually use, eliminating concerns over insufficient computing capacity to meet customer needs.\nIf you require the scaling process to happen automatically, the AWS service to use is Amazon EC2 Auto Scaling.\nAmazon EC2 Auto Scaling functions much like having a dynamic staffing system in a busy coffee shop. Just as a coffee shop might call in extra baristas on busy mornings to keep the line moving, Amazon EC2 Auto Scaling automatically adds or removes EC2 instances in response to application demands. This ensures that your application remains available without unnecessary delays, akin to a customer facing a long wait due to only one barista being available.\nWithin Amazon EC2 Auto Scaling, there are two main strategies:\n\nDynamic Scaling: Adjusts the number of EC2 instances as demand on your application increases or decreases.\nPredictive Scaling: Uses forecasting to predict demand and schedules the appropriate number of EC2 instances in advance.\n\nFor instance, imagine you are launching an application on Amazon EC2. Initially, you set your Auto Scaling group with a minimum of one EC2 instance, ensuring that there is always at least one instance running. You might also set a desired capacity of two instances, although only one is strictly necessary for operation.\nFurthermore, you can specify a maximum capacity for your Auto Scaling group—say, four instances. This cap allows your application to scale out in response to increased demand but ensures it does not exceed four instances, helping manage costs effectively.\nBy leveraging Amazon EC2 Auto Scaling, you pay only for the EC2 instances you use, precisely when you use them. This approach not only optimizes your expenditure but also ensures your architecture can provide the best possible customer experience by adapting to traffic fluctuations without manual intervention.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>**Scaling Amazon EC2**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Elastic-Load-Balancing.html",
    "href": "module-2-notes/Elastic-Load-Balancing.html",
    "title": "5  Directing Traffic with Elastic Load Balancing",
    "section": "",
    "text": "Elastic Load Balancing (ELB) is an AWS service designed to automatically distribute incoming application traffic across multiple resources, such as Amazon EC2 instances. It functions as the central point of contact for all incoming web traffic to your Auto Scaling group. As your application scales by adding or removing EC2 instances based on traffic volume, the ELB routes these incoming requests first, then distributes them across multiple instances. This ensures that no single instance bears too much load, maintaining an even distribution that optimises resource use and enhances application performance.\nAlthough Elastic Load Balancing and Amazon EC2 Auto Scaling are distinct services, they are often used together to enhance the performance and availability of applications running on Amazon EC2. They collectively ensure that applications can handle high traffic loads efficiently without compromising on speed or availability.\nExample of Elastic Load Balancing:\nConsider Elastic Load Balancing as the organisational force in a bustling coffee shop. During periods of low demand, only a few registers need to be open to manage the customer flow effectively. This scenario is akin to having a smaller number of EC2 instances during quieter periods. Each register (or instance) has just enough customers (or traffic) to stay efficiently active without being idle.\nAs the day progresses and customer numbers increase, the coffee shop responds by opening more registers. A shop employee, acting much like a load balancer, directs customers to registers, ensuring that the workload is evenly spread across all open registers. This prevents any single register from becoming overwhelmed, much like how ELB prevents any single EC2 instance from becoming overburdened during high-demand periods.\nBy using Elastic Load Balancing, you ensure that your application’s traffic is managed as efficiently as a well-organised coffee shop, scaling resources up or down as needed and directing traffic to where it can be handled most effectively. This system not only improves the overall user experience but also optimises operational efficiency and resource use.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>**Directing Traffic with Elastic Load Balancing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Messaging-and-Queuing.html",
    "href": "module-2-notes/Messaging-and-Queuing.html",
    "title": "6  Messaging and Queuing",
    "section": "",
    "text": "6.1 Monolithic Applications\nA monolithic application is structured as a single, indivisible unit. This approach is traditional, where all components of the application, such as the user interface, business logic, database interactions, and other functions, are tightly integrated into a single software package. In a monolithic architecture:\nExample: Imagine a web application where the user interface, server-side logic, and database management are all handled by a single platform. If you need to update the database schema, the entire application might need to be tested and redeployed.\nDrawbacks: If one component of a monolithic application fails, it can jeopardise the entire system’s stability and availability because all components are interdependent.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Messaging and Queuing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Messaging-and-Queuing.html#monolithic-applications",
    "href": "module-2-notes/Messaging-and-Queuing.html#monolithic-applications",
    "title": "6  Messaging and Queuing",
    "section": "",
    "text": "All components share the same memory space and resources.\nUpdates or changes to any single component often require redeploying the entire application.\nScalability can be challenging, as scaling the application typically means scaling the entire system, even if only one part requires more resources.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Messaging and Queuing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Messaging-and-Queuing.html#microservices",
    "href": "module-2-notes/Messaging-and-Queuing.html#microservices",
    "title": "6  Messaging and Queuing",
    "section": "6.2 Microservices",
    "text": "6.2 Microservices\nMicroservices architecture breaks down an application into a collection of smaller, interconnected services, each performing a specific business function. These services are:\n\nLoosely coupled: Each service functions independently. Failure in one area does not impact the availability of others.\nHighly maintainable and testable: Services can be deployed, updated, redeployed, and scaled independently.\nOrganized around business capabilities: Each service corresponds to a business goal and can be developed by a team that understands that goal deeply.\n\nExample: In a retail application, microservices might include separate services for user accounts, product catalog management, order processing, and payment handling. Each service interacts with the others through well-defined interfaces, usually REST APIs.\nAdvantages: This architecture enhances the resilience of the application. If one service fails, the others continue to operate, potentially only reducing the functionality temporarily rather than causing a total application failure.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Messaging and Queuing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Messaging-and-Queuing.html#aws-and-microservices",
    "href": "module-2-notes/Messaging-and-Queuing.html#aws-and-microservices",
    "title": "6  Messaging and Queuing",
    "section": "6.3 AWS and Microservices",
    "text": "6.3 AWS and Microservices\nAWS supports microservices through various managed services that reduce the overhead of handling the infrastructure. Key AWS services that facilitate microservices include:\n\n6.3.1 Amazon Simple Notification Service (SNS)\n\nA managed publish/subscribe service that decouples microservices by allowing them to publish or subscribe to notifications. It ensures that messages are pushed to multiple subscribers and can trigger functions, HTTP endpoints, or email notifications.\nImagine you’re at a party and you have an announcement to make. Instead of going to each person individually and repeating your message, you decide to use a loudspeaker. This way, everyone at the party can hear your message at the same time. This is similar to what a publish/subscribe (pub/sub) messaging service does. It’s like a digital loudspeaker for your applications.\nIn the world of software, we often have smaller, independent applications called microservices. These microservices need to talk to each other, but we don’t want them to do so directly because it can get very complicated very quickly. So, we use a pub/sub messaging service.\nHere’s how it works:\n\nWhen a microservice has a new piece of information to share (like our party announcement), it publishes a message to the messaging service.\nOther microservices that are interested in this information subscribe to these messages. Just like how people at the party would pay attention to the announcement.\nThe messaging service then makes sure that all the subscribers get the message. It’s like ensuring everyone at the party hears the announcement.\n\nThis system allows our microservices to remain decoupled, meaning they can operate independently without knowing specifics about each other, just like how people at the party can mingle independently without needing to know everyone’s details.\nLastly, these messages can trigger different actions, like starting up other functions, sending data to a website (HTTP endpoints), or even sending out email notifications. It’s like if your party announcement was to start a dance-off, head to the buffet, or check your email for a surprise!",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Messaging and Queuing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Messaging-and-Queuing.html#amazon-simple-queue-service-sqs",
    "href": "module-2-notes/Messaging-and-Queuing.html#amazon-simple-queue-service-sqs",
    "title": "6  Messaging and Queuing",
    "section": "6.4 Amazon Simple Queue Service (SQS)",
    "text": "6.4 Amazon Simple Queue Service (SQS)\n\nThis is a managed message queuing service used for storing messages in transit between computers. By decoupling components, SQS allows individual components to scale independently, handle spikes, and ensure no message is lost or duplicated.\nLet’s imagine you’re in a busy post office. There are many people trying to send letters and packages, and the post office needs to make sure that everything gets delivered correctly. This is similar to what Amazon Simple Queue Service (SQS) does.\nHere’s how it works:\n\nWhen a computer (or in our analogy, a person) has a message (or a letter) to send, it gives it to SQS (the post office).\nSQS stores the message in a queue, just like how a post office would store letters and packages until they’re ready to be delivered.\nAnother computer can then come and pick up the message from the queue when it’s ready, just like how a mail carrier would pick up letters and packages from the post office to deliver them.\n\nThis system allows the computers (or people in our analogy) to work independently. They don’t need to know anything about each other, just like how you don’t need to know the mail carrier who will deliver your letter.\nSQS also helps handle spikes in traffic. If lots of messages are being sent at once, SQS can store them all and deliver them when it’s able to, just like how a post office can store many letters and packages during a busy holiday season.\nFinally, SQS makes sure that no message is lost or duplicated. It’s like how a post office makes sure that every letter and package is delivered exactly once, and that nothing gets lost in transit.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>**Messaging and Queuing**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Additional-Compute-Services.html",
    "href": "module-2-notes/Additional-Compute-Services.html",
    "title": "7  Additional Compute Services",
    "section": "",
    "text": "7.1 Serverless computing\n“Serverless” might sound like there are no servers involved, but in reality, your code still runs on servers. The key difference is that you don’t have to worry about setting up or managing these servers. This allows you to shift your focus from server maintenance to innovating new products and features.\nOne of the major advantages of serverless computing is its ability to automatically scale your applications. It adjusts the capacity of your applications by modifying units of consumption, such as throughput and memory, providing you with the flexibility you need.\nThis is quite different from services like Amazon EC2, where you’re given the ability to run virtual servers in the cloud. With EC2, you’re responsible for provisioning instances (virtual servers), uploading your code, and continuously managing these instances while your application is running. In contrast, serverless computing takes care of these tasks for you, freeing up your time and resources.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>**Additional Compute Services**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Additional-Compute-Services.html#aws-lambda",
    "href": "module-2-notes/Additional-Compute-Services.html#aws-lambda",
    "title": "7  Additional Compute Services",
    "section": "7.2 AWS Lambda",
    "text": "7.2 AWS Lambda\nAWS Lambda is a service provided by AWS that embodies the concept of serverless computing. It allows you to run your code without the need to set up or manage servers.\nThe beauty of AWS Lambda is its pay-as-you-go model. You’re only billed for the compute time your code consumes, not a second more. This means you’re not paying for idle time, and costs are kept to a minimum. Plus, it’s versatile. You can run code for virtually any type of application or backend service, all without any administrative tasks.\nLet’s consider a simple example. Suppose you have a Lambda function set up to automatically resize images uploaded to the AWS Cloud. The function springs into action the moment a new image is uploaded.\nHere’s how AWS Lambda works :\n\nYou upload your code to Lambda.\nYou configure your code to be triggered by an event source. This could be anything from AWS services to mobile applications or HTTP endpoints.\nLambda springs into action and runs your code, but only when triggered.\nYou’re billed solely for the compute time you consume. So, in our image resizing example, you’d only pay for the compute time used when new images are uploaded and the resizing function is triggered.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>**Additional Compute Services**</span>"
    ]
  },
  {
    "objectID": "module-2-notes/Additional-Compute-Services.html#containers",
    "href": "module-2-notes/Additional-Compute-Services.html#containers",
    "title": "7  Additional Compute Services",
    "section": "7.3 Containers",
    "text": "7.3 Containers\n\nContainers are like little boxes where you can pack up your application’s code and all the things it needs to run (these are called dependencies). This is great because it means your application will run the same way no matter where you put it, just like how a toy packed in a box can be played with the same way no matter where you open it.\nAWS allows you to build and run these containerized applications. Containers are great for when you need your application to be secure, reliable, and scalable.\nNow, let’s dive a bit deeper into how containers work:\nScenario 1: One host with multiple containers \nImagine you’re a developer at a company. The environment on your computer is different from the environment on the computers used by the IT operations staff. To make sure that your application’s environment stays the same no matter where it’s deployed, you decide to use a container. This is like packing your lunch in a lunchbox to make sure it stays the same no matter where you eat it. This approach helps reduce time spent debugging applications and diagnosing differences in computing environments.\nScenario 2: Tens of hosts with hundreds of containers \nWhen running containerized applications, it’s important to think about scalability. Imagine you’re not just managing one lunchbox, but tens of lunchboxes with hundreds of lunches inside. Or maybe even hundreds of lunchboxes with thousands of lunches! At this scale, think about how much time it might take for you to check each lunch (monitor memory usage), make sure no lunches are stolen (security), keep track of what’s in each lunchbox (logging), and so on.\nAmazon offers several services for containerized applications, including:\nAmazon Elastic Container Service (Amazon ECS): This is a highly scalable, high-performance container management system that allows you to run and scale containerized applications on AWS. Amazon ECS supports Docker containers, a software platform that lets you build, test, and deploy applications quickly. AWS supports both the open-source Docker Community Edition and the subscription-based Docker Enterprise Edition. With Amazon ECS, you can use API calls to launch and stop Docker-enabled applications.\nAmazon Elastic Kubernetes Service (Amazon EKS): This is a fully managed service that lets you run Kubernetes, an open-source software that allows you to deploy and manage containerized applications at scale, on AWS. A large community of volunteers maintains Kubernetes, and AWS actively collaborates with this community. As new features and functionalities are released for Kubernetes applications, you can easily apply these updates to your applications managed by Amazon EKS.\nAWS Fargate: This is a serverless compute engine for containers that works with both Amazon ECS and Amazon EKS. When using AWS Fargate, you don’t need to provision or manage servers. AWS Fargate takes care of your server infrastructure, allowing you to focus more on innovating and developing your applications. Plus, you only pay for the resources required to run your containers.",
    "crumbs": [
      "Module 2",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>**Additional Compute Services**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/AWS-Global-Infrastructure.html",
    "href": "module-3-notes/AWS-Global-Infrastructure.html",
    "title": "8  AWS Global Infrastructure",
    "section": "",
    "text": "8.1 Business factors that influence the choice of a region.\nWhen determining the right region for your services, data, and applications, consider the following four business factors:",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**AWS Global Infrastructure**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/AWS-Global-Infrastructure.html#business-factors-that-influence-the-choice-of-a-region.",
    "href": "module-3-notes/AWS-Global-Infrastructure.html#business-factors-that-influence-the-choice-of-a-region.",
    "title": "8  AWS Global Infrastructure",
    "section": "",
    "text": "8.1.1 Compliance with data governance and legal requirements\nYou may need to keep your data in particular locations, depending on the policies and location of your company. For example, if your organisation demands that all data remain in the UK, you would select the London Region. Not every company has location-specific data regulations. If your location is not governed by compliance or regulatory standards, focus on the other issues. For example, if you need to store data within South Africa, select the Cape Town Region. However, most businesses are not subject to such severe laws. If compliance isn’t a concern, consider the other factors.\n\n\n8.1.2 Proximity to customers\nChoosing a region close to your customers helps deliver content faster. For example, if your company is based in Johannesburg and many of your clients are in Nairobi, you may run your infrastructure in the Cape Town Region to be close to headquarters, while running your apps in the Nairobi Region. The proximity to your consumer base is critical because of latency. If most of your customers are in Nairobi, consider running from the Nairobi region. While you can operate from South Africa, the time it takes for data to travel between South Africa and Kenya will always be a factor. Locating close to your customer base is usually the best decision.\n\n\n8.1.3 Availability of services within a Region\nSometimes the nearest region might not offer all the features you need. AWS is continuously innovating and adding new services, but rolling out new services worldwide requires physical infrastructure updates in each region. For example, if your developers want to use Amazon Braket (AWS’s quantum computing platform) but it isn’t available in your local region, they’ll need to run it in one that is. While we may expect these services to eventually be available in all countries, availability may be the deciding factor for you right now.\n\n\n8.1.4 Pricing\nConsider deploying applications in both South Africa and Kenya. Due to Kenya’s tax system, it may cost 50% more to run the same workload in the Nairobi Region than in the Cape Town Region. Pricing varies by area due to factors such as tax rates and operational costs. For example, running the same workload in Nairobi could cost much more than in Cape Town. AWS pricing is transparent, with various price sheets for each location. If budget is a top priority, you may choose to operate in South Africa even if the customers are in Kenya.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**AWS Global Infrastructure**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/AWS-Global-Infrastructure.html#availability-zones",
    "href": "module-3-notes/AWS-Global-Infrastructure.html#availability-zones",
    "title": "8  AWS Global Infrastructure",
    "section": "8.2 Availability Zones",
    "text": "8.2 Availability Zones\nWhen it comes to hosting your application, you might be concerned about relying on a single building. This concern is valid as a single building can fail due to various unforeseen circumstances. If your business needs to be disaster-proof, it can’t operate from just one location. AWS acknowledges this fact, and that’s why AWS Regions are not confined to a single location.\nAWS regions and data Centers\nAWS has numerous data centers scattered across the globe. Each AWS Region comprises multiple data centers. A single data center or a group of data centers is referred to as an Availability Zone (AZ) by AWS.\nAn Availability Zone is a single data center or a group of data centers within a region. Availability Zones are located tens of miles apart from each other. This is close enough to have low latency (the time between when content requested and received) between Availability Zones. However, if a disaster occurs in one part of the region, they are distant enough to reduce the chance that multiple Availability Zones are affected.\nEach Availability Zone consists of one or more distinct data centers with redundant power, networking, and connectivity. When you launch an Amazon EC2 instance, it initiates a virtual machine on physical hardware installed in an Availability Zone. This means each AWS Region consists of multiple isolated and physically separate Availability Zones within a geographic Region.\nImportance of separation\nAvailability Zones are not built adjacent to each other. If a large-scale incident like a natural disaster were to occur, you could lose connectivity to everything in that Availability Zone.\nIf you only run one EC2 instance, it only operates in one building or one Availability Zone. If a large-scale disaster occurs, will your application still be able to run and serve your business? The solution to this is to run multiple EC2 instances.\nThe key point is not to run them in the same building or even on the same street. Push them as far apart as you can before the speed of light limits you if you still want low latency communication. The speed of light allows us to move these Availability Zones tens of miles apart from each other and still maintain single-digit millisecond latency between these Availability Zones. Now, if a disaster strikes, your application continues to operate because this disaster only affected some of your capacity, not all.\nBest practices\nAs a best practice with AWS, we always recommend you run across at least two Availability Zones in a Region. This means redundantly deploying your infrastructure in two different AZs.\nBut there’s more to Regions than just places to run EC2. Many of the AWS services run at the Region level, meaning they run synchronously across multiple AZs without any additional effort on your part.For example, the Elastic Load Balancer (ELB) we discussed previously is actually a regional construct. It operates across all Availability Zones, communicating with the EC2 instances running in a specific Availability Zone.\nA Region consists of three or more Availability Zones.For example, the South America (São Paulo) Region is sa-east-1. It includes three Availability Zones: sa-east-1a, sa-east-1b, and sa-east-1c.\nRegional services are, by definition, already highly available at no additional cost or effort on your part. So, as you plan for high availability, any service listed as a regionally scoped service will already have that box checked. In the next section, we will look at going outside the Regions for additional solutions.\nAdditional links\nAWS global infrastructure\nRegions and Availability Zones",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>**AWS Global Infrastructure**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html",
    "href": "module-3-notes/Edge-Locations.html",
    "title": "9  Edge Locations",
    "section": "",
    "text": "9.1 Building Satellite Stores\nConsider a scenario where you have a thriving customer base in a new city. You can establish a satellite store to cater to these customers. From an IT perspective, if you have customers in Nairobi who need access to your data, but the data is hosted out of the London Region, you can place or cache a copy locally in Nairobi instead of having all the Nairobi-based customers send requests all the way to London to access the data.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html#content-delivery-networks-cdns",
    "href": "module-3-notes/Edge-Locations.html#content-delivery-networks-cdns",
    "title": "9  Edge Locations",
    "section": "9.2 Content Delivery Networks (CDNs)",
    "text": "9.2 Content Delivery Networks (CDNs)\nCaching copies of data closer to the customers all around the world employs the concept of content delivery networks, or CDNs. CDNs are commonly used, and on AWS, our CDN is called Amazon CloudFront.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html#amazon-cloudfront",
    "href": "module-3-notes/Edge-Locations.html#amazon-cloudfront",
    "title": "9  Edge Locations",
    "section": "9.3 Amazon CloudFront",
    "text": "9.3 Amazon CloudFront\nAmazon CloudFront is a service that helps deliver data, video, applications, and APIs to customers globally with low latency and high transfer speeds. Amazon CloudFront uses what are called Edge locations, all around the world, to help speed up communication with users, no matter where they are.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html#edge-locations",
    "href": "module-3-notes/Edge-Locations.html#edge-locations",
    "title": "9  Edge Locations",
    "section": "9.4 Edge Locations",
    "text": "9.4 Edge Locations\nAn edge location is a site that Amazon CloudFront uses to store cached copies of your content closer to your customers for faster delivery.\nEdge locations are separate from Regions, so you can push content from inside a Region to a collection of Edge locations around the world to accelerate communication and content delivery. AWS Edge locations also run more than just CloudFront. They run a domain name service, or DNS, known as Amazon Route 53, helping direct customers to the correct web locations with reliably low latency.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html#aws-outposts",
    "href": "module-3-notes/Edge-Locations.html#aws-outposts",
    "title": "9  Edge Locations",
    "section": "9.5 AWS Outposts",
    "text": "9.5 AWS Outposts\nBut what if your business wants to use AWS services inside their own building? AWS can do that for you. AWS Outposts is a service where AWS will essentially install a fully operational mini Region right inside your own data center. That’s owned and operated by AWS, using 100% of AWS functionality, but isolated within your own building. It’s not a solution most customers need, but if you have specific problems that can only be solved by staying in your own building, AWS Outposts can help.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/Edge-Locations.html#key-points",
    "href": "module-3-notes/Edge-Locations.html#key-points",
    "title": "9  Edge Locations",
    "section": "9.6 Key Points",
    "text": "9.6 Key Points\nTo summarise, here are the key points:\n\nRegions are geographically isolated areas where you can access services needed to run your enterprise.\nRegions contain Availability Zones that allow you to run across physically separated buildings, tens of miles of separation, while keeping your application logically unified. Availability Zones help you solve high availability and disaster recovery scenarios, without any additional effort on your part.\nAWS Edge locations run Amazon CloudFront to help get content closer to your customers, no matter where they are in the world.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>**Edge Locations**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/How-to-Provision-AWS .html",
    "href": "module-3-notes/How-to-Provision-AWS .html",
    "title": "10  How to Provision AWS",
    "section": "",
    "text": "10.0.1 Understanding APIs\nAn API, or Application Programming Interface, provides predetermined ways for you to interact with AWS services. You can invoke or call these APIs to provision, configure, and manage your AWS resources.\nFor instance, launching an EC2 instance or creating an AWS Lambda function would each involve different requests and different API calls to AWS. You can use various tools like the AWS Management Console, the AWS Command Line Interface (CLI), the AWS Software Development Kits (SDKs), or AWS CloudFormation to create requests to send to AWS APIs to create and manage AWS resources.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**How to Provision AWS**</span>"
    ]
  },
  {
    "objectID": "module-3-notes/How-to-Provision-AWS .html#interacting-with-aws-services-using-managed-tools",
    "href": "module-3-notes/How-to-Provision-AWS .html#interacting-with-aws-services-using-managed-tools",
    "title": "10  How to Provision AWS",
    "section": "10.1 Interacting with AWS Services Using Managed Tools",
    "text": "10.1 Interacting with AWS Services Using Managed Tools\nIn addition to the AWS Management Console, the Command Line Interface (CLI), and the Software Development Kits (SDKs), AWS provides managed tools like AWS Elastic Beanstalk and AWS CloudFormation to help you provision and manage your AWS environment.\n\n10.1.1 AWS Elastic Beanstalk\nAWS Elastic Beanstalk is a service that assists you in provisioning Amazon EC2-based environments. Instead of navigating through the console or writing multiple commands to build out your network, EC2 instances, scaling, and Elastic Load Balancers, you can provide your application code and desired configurations to the AWS Elastic Beanstalk service. This service then uses that information to build out your environment for you.\nAWS Elastic Beanstalk also simplifies the process of saving environment configurations, allowing them to be deployed again easily. It offers the convenience of not having to provision and manage all these components separately, while still providing visibility and control of the underlying resources. This allows you to focus on your business application, not the infrastructure.\n\n\n10.1.2 AWS CloudFormation\nAWS CloudFormation is another service that helps create automated and repeatable deployments. It is an infrastructure-as-code tool that allows you to define a wide variety of AWS resources in a declarative way using JSON or YAML text-based documents, known as CloudFormation templates.\nA declarative format allows you to define what you want to build without specifying the details of exactly how to build it. CloudFormation lets you define what you want, and the CloudFormation engine takes care of the details, calling APIs to get everything built out.\nCloudFormation isn’t just limited to EC2-based solutions. It supports many different AWS resources, including storage, databases, analytics, machine learning, and more. Once you define your resources in a CloudFormation template, CloudFormation parses the template and begins provisioning all the resources you defined in parallel.\nCloudFormation manages all the calls to the backend AWS APIs for you. You can run the same CloudFormation template in multiple accounts or multiple regions, and it will create identical environments across them. This automated process reduces the room for human error.\n\n\n10.1.3 Recap\nTo recap, the AWS Management Console is great for learning and providing a visual for the user. However, it is a manual tool and not the best option for automation. Instead, you can use the CLI to script your interactions with AWS using the terminal, or the SDKs to write programs to interact with AWS. For a more managed approach, you can use tools like AWS Elastic Beanstalk or AWS CloudFormation.",
    "crumbs": [
      "Module 3",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**How to Provision AWS**</span>"
    ]
  },
  {
    "objectID": "module-4-notes/2-Connectivity-to-AWS.html",
    "href": "module-4-notes/2-Connectivity-to-AWS.html",
    "title": "11  Connectivity to AWS",
    "section": "",
    "text": "When we talk about connectivity in Amazon Web Services (AWS), we are referring to the different ways in which your resources within the AWS Cloud can communicate with each other, as well as the different ways in which they can connect to the outside world. Connectivity is a fundamental concept in cloud computing because it ensures that the various components of your application or service can interact seamlessly and securely.\nOne of the core components that facilitates connectivity within AWS is the Virtual Private Cloud (VPC). But, before we get into what a VPC is and how it works, we should first understand why we need it.\n\n11.0.1 Why a VPC?\nIn traditional IT infrastructures, companies store their applications and data in their own physical data centres. These data centres have network limits and security mechanisms in place to ensure that only permitted users and systems have access to the resources they contain. When migrating to the cloud, companies require the same type of control and security. This is where VPCs come into play.\nA VPC, or Virtual Private Cloud, allows you to build a conceptually isolated area of the AWS Cloud from which you can launch AWS resources in a virtual network that you choose. This isolation provides the essential security and control, similar to that found in a typical data centre, but with the extra benefit of cloud scalability and flexibility.\n\n\n11.0.2 The Structure of a VPC\nA virtual private cloud (VPC) gives you the ability to construct your own private IP range for your Amazon Web Services (AWS) resources. Within your VPC, you may store services such as EC2 instances and ELBs.\nNow you don’t just throw your resources into a VPC and move on. You assign them to distinct subnets. Subnets are chunks of IP addresses in your VPC that enable you to combine resources together. Subnets, along with networking rules we will cover later, control whether resources are either publicly or privately available. There are actually ways you can control what traffic gets into your VPC at all. For some VPCs, you might have internet-facing resources that the public should be able to reach, like a public website.\nHowever, in other cases, you may have resources that should only be accessible if someone is logged into your private network. This could be internal services, such as an HR application or a backend database. First, let’s look at public-facing resources.\n\n\n11.0.3 The Coffee Shop Analogy\nImagine your VPC as a coffee shop. Just as a coffee shop needs a way for customers to enter and exit, your VPC needs ways for data to flow in and out. Depending on the type of customers (or data) and their needs, you’ll have different types of entrances.\n\n\n11.0.4 Internet Gateway\nTo allow traffic from the public internet to flow into and out of your VPC, you must attach an Internet Gateway (IGW).\nAn Internet Gateway is like a doorway that is open to the public. Think of it like a coffee shop. Customers can’t get in and get their coffee without a front door, so we’ll have to install one, and people will be able to enter and exit our cafe through it. In this case, the front door functions similarly to an Internet gateway. Without it, no one may access the resources stored within your VPC.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n11.0.5 Virtual Private Gateway\nNext, let’s talk about a VPC with all internal private resources. We don’t want just anyone from anywhere to be able to reach these resources. So we don’t want an Internet Gateway attached to our VPC. Instead, we want a private gateway that only allows people in if they are coming from an approved network, not the public internet. This private doorway is called a Virtual Private Gateway, and it allows you to create a VPN connection between a private network, like your on-premises data centre or internal corporate network to your VPC.\nTo relate this back to the coffee shop, this would be like having a private bus route going from my building to the coffee shop. If I want to get coffee, I first must badge into the building, thus authenticating my identity, and then I can take the secret bus route to the internal coffee shop that only people from my building can use. So if you want to establish an encrypted VPN connection to your private internal AWS resources, you would need to attach a Virtual Private Gateway to your VPC.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n11.0.6 AWS Direct Connect\nNow the problem with our super secret bus route is that it still uses the open road. It’s susceptible to traffic jams and slowdowns caused by the rest of the world going about their business. The same thing is true for VPN connections. They are private and encrypted, but they still use a regular internet connection that has bandwidth that is being shared by many people using the internet.\nSo what I’ve done to make things more reliable and less susceptible to slowdowns is I made a totally separate magic doorway that leads directly from the studio into the coffee shop. No one else driving around on the road can slow me down because this is my direct doorway; no one else can use it. What, did you not have a secret magic doorway into your favourite coffee shop? All right, moving on. The point is you still want a private connection, but you want it to be dedicated and shared with no one else. You want the lowest amount of latency possible with the highest amount of security possible.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nWith AWS, you can achieve that using what is called AWS Direct Connect. Direct Connect allows you to establish a completely private, dedicated fibre connection from your data centre to AWS. You work with a Direct Connect partner in your area to establish this connection because, like my magic doorway, AWS Direct Connect provides a physical line that connects your network to your AWS VPC. This can help you meet high regulatory and compliance needs, as well as sidestep any potential bandwidth issues. It’s also important to note that one VPC might have multiple types of gateways attached for multiple types of resources all residing in the same VPC, just in different subnets.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Connectivity to AWS</span>"
    ]
  },
  {
    "objectID": "module-4-notes/3-Subnets-and-NACL.html",
    "href": "module-4-notes/3-Subnets-and-NACL.html",
    "title": "12  Subnets and Network Access Control Lists",
    "section": "",
    "text": "Your Virtual Private Cloud (VPC) is like a fortress; nothing enters or exits without your explicit permission. Think of your virtual private cloud (VPC) as a high-security building with distinct rooms (subnets), and rigorous security measures (Network Access Control Lists, or NACLs), regulating who can enter and exit each room. Let’s explore how this works and why it’s essential for your network security.\n\n12.0.1 Background and analogy\nImagine you own a large, secure building (your VPC). This building has multiple rooms, each with different purposes and levels of access:\n\nPublic room: This room is like the lobby of your building, accessible to anyone who walks in from the street (internet). This is where you place resources like your website, which need to be accessible to the public.\nPrivate room: This room is deep inside your building, accessible only to authorised personnel. This is where you store sensitive information, like customer databases.\n\nTo control access to these rooms, you have two types of security personnel:\n\nNetwork ACLs: These are like passport control officers at the border of each room (subnet). They check the credentials of every person (data packet) entering or leaving the room.\nSecurity groups: These are like doormen at the entrance of each office (EC2 instance). They ensure only authorised individuals can enter specific offices.\n\n\n\n12.0.2 Subnets\nA subnet is a section of your VPC where you can group resources based on security or operational needs. Subnets can be public or private:\n\nPublic Subnets: These contain resources that need to be accessible by the public, such as a website.\nPrivate Subnets: These contain resources that should only be accessible through your private network, like a database.\n\nFor example, you might have an application where the web servers are in a public subnet, and they communicate with databases in a private subnet.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n12.0.3 Network Traffic in a VPC\nWhen a customer requests data from an application hosted in the AWS Cloud, this request is sent as a packet (a unit of data). This packet enters the VPC through an internet gateway and must pass through several security checks before reaching its destination.\n\n\n12.0.4 Network ACLs\nA Network Access Control List (NACL) is a virtual firewall that controls inbound and outbound traffic at the subnet level:\n\nInbound Rules: These rules control incoming traffic.\nOutbound Rules: These rules control outgoing traffic.\n\nThink of a NACL as a passport control officer at the border of a country. This officer checks travelers’ credentials when they are entering and exiting the country. If the traveler (data packet) is on the approved list, they can pass. If not, they are denied entry.\nEach AWS account includes a default NACL. By default, this NACL allows all inbound and outbound traffic, but you can modify it to add specific rules. For custom NACLs, all traffic is denied until you add rules to allow specific traffic.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n12.0.5 Security Groups\nWhile NACLs control traffic at the subnet level, Security Groups control traffic at the instance level:\n\nEach EC2 instance has an associated security group.\nBy default, a security group blocks all incoming traffic and allows all outgoing traffic.\nYou can modify security group rules to allow specific types of traffic, such as web traffic (HTTP/HTTPS).\n\nIf NACLs are like passport control, security groups are like doormen at the entrance of an office. They ensure that only authorised traffic can enter the instance.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n12.0.6 Example Scenario\nLet’s walk through an example of how a packet travels from one instance to another in a different subnet:\n\nInstance A (Source):\n\nThe packet leaves Instance A and passes through its security group. Since security groups allow all outbound traffic by default, the packet can leave without any issues.\nThe packet then reaches the boundary of the subnet and must pass through the NACL. If the packet is allowed by the NACL, it can proceed.\n\nSubnet Boundary:\n\nThe packet crosses into the target subnet and must pass through the NACL at this boundary. If it’s allowed, it can enter the subnet.\n\nInstance B (Destination):\n\nThe packet reaches Instance B and must pass through its security group. If the security group allows the traffic, the packet reaches its destination.\n\nReturn Traffic:\n\nWhen the packet returns, the security group of Instance B allows it to leave (default behavior).\nThe NACL at the subnet boundary checks the packet again.\nThe packet crosses into the original subnet and passes through its NACL.\nFinally, the security group of Instance A recognizes the returning packet and allows it in.\n\n\nBy using both NACLs and security groups, AWS ensures a robust and layered security approach for your VPC. NACLs provide broad subnet-level security, while security groups offer fine-grained instance-level security. Together, they help keep your network secure and functional, much like the combined efforts of passport control officers and doormen in a high-security building.\nThis layered security model is crucial for protecting your AWS infrastructure and ensuring that only authorised traffic can access your resources.\nFor more detailed information and additional training on AWS security, be sure to follow the provided links and enhance your knowledge to secure your infrastructure effectively.\n\n\n12.0.7 Understanding Stateless and Stateful Packet Filtering in AWS\nEnsuring the security of your network traffic is essential when working with AWS. Two fundamental mechanisms that AWS provides for this purpose are Network Access Control Lists (NACLs) and Security Groups. These tools are used to filter traffic, but they operate in different ways: stateless packet filtering and stateful packet filtering.\n\n\n12.0.8 Stateless Packet Filtering with NACLs\nNetwork ACLs perform stateless packet filtering, meaning they do not remember previous decisions and evaluate each packet crossing the subnet boundary individually.\nAnalogy: Imagine you’re at an airport trying to enter a different country. Each time you arrive at the passport control (network ACL), the officer checks your documents regardless of whether you’ve entered before. Similarly, NACLs check each packet against their rules for both inbound and outbound traffic.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\nInbound Traffic: When a data packet (traveller) arrives at the subnet border (country’s border), the NACL checks its credentials (source IP, protocol, port) against a list of rules. If approved, the packet is allowed in; otherwise, it’s denied entry.\nOutbound Traffic: When a packet leaves the subnet, the NACL checks it again, ensuring it meets the outbound rules before allowing it to exit.\n\nExample: Consider a request sent from an Amazon EC2 instance to the internet. When the response packet returns, the NACL doesn’t remember the initial request. It evaluates the response packet against its rules to decide whether to allow it into the subnet.\n\n\n12.0.9 Stateful Packet Filtering with Security Groups\nSecurity Groups perform stateful packet filtering, which means they remember previous decisions regarding incoming packets and allow responses based on this memory.\nAnalogy: Picture an apartment building with a door attendant (security group) in the lobby. When guests (packets) arrive, the attendant checks a list to ensure they’re allowed entry. Once inside, the attendant remembers these guests and does not check the list again when they exit.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\nInbound Traffic: By default, a security group denies all inbound traffic. You can add rules to allow specific types of traffic, such as HTTP or SSH.\nOutbound Traffic: By default, a security group allows all outbound traffic. You can modify these rules if needed.\n\nExample: Suppose an EC2 instance sends a request to the internet. When the response packet comes back, the security group remembers the initial request and allows the response to enter, even if the inbound rules do not explicitly allow it.\n\n\n12.0.10 Comparing NACLs and Security Groups\nUnderstanding the differences between NACLs and Security Groups is crucial:\n\nNetwork ACLs:\n\nStateless: They do not remember past decisions.\nSubnet-Level: Control traffic at the subnet level.\nRules: Evaluate both inbound and outbound traffic separately.\n\nSecurity Groups:\n\nStateful: They remember past decisions.\nInstance-Level: Control traffic at the instance level.\nRules: Allow all outbound traffic by default and evaluate inbound traffic based on rules.\n\n\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n12.0.11 VPC Component Recap\nHere is a quick recap of the VPC components we’ve discussed:\n\nSubnets: Segments within a VPC that can be public or private, controlling access based on security needs.\nNetwork ACLs: Stateless firewalls at the subnet level, controlling inbound and outbound traffic.\nSecurity Groups: Stateful firewalls at the instance level, controlling inbound traffic and allowing outbound traffic by default.\nInternet Gateway: A gateway that allows communication between the VPC and the internet, used by public subnets.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>**Subnets and Network Access Control Lists**</span>"
    ]
  },
  {
    "objectID": "module-4-notes/4-Global-Networking.html",
    "href": "module-4-notes/4-Global-Networking.html",
    "title": "13  Global Networking",
    "section": "",
    "text": "13.0.1 Global Networking in AWS\nUnderstanding how customers interact with your AWS infrastructure is essential for optimising performance and ensuring seamless user experiences. This section will guide you through the concepts of Domain Name System (DNS) and the key AWS services that facilitate efficient global networking.\n\n\n13.0.2 Domain Name System (DNS)\nImagine AnyCompany has a website hosted in the AWS Cloud. When customers enter the web address into their browser, they can access the website thanks to DNS resolution. DNS resolution involves translating a domain name into an IP address, akin to looking up a number in a phone book.\n\n\n13.0.3 How DNS Works\n\nCustomer Request: When you enter the domain name into your browser, the request is sent to a customer DNS resolver.\nDNS Resolver Query: The customer DNS resolver queries the company DNS server for the IP address associated with AnyCompany’s website.\nResponse: The company DNS server responds with the IP address, such as 192.0.2.0, allowing the browser to access the website.\n\n\n\n13.0.4 Amazon Route 53\nAmazon Route 53 is AWS’s DNS web service, offering highly available and scalable domain name resolution. It translates website names into IP addresses, enabling seamless navigation across the internet.\n\n\n13.0.5 Key Features of Route 53\n\nDomain Name Registration: You can register and manage domain names directly within Route 53.\nTraffic Routing: Route 53 can direct traffic using various routing policies, such as latency-based routing, geolocation DNS, geoproximity, and weighted round robin.\nIntegration with AWS Services: Route 53 connects user requests to AWS infrastructure, like Amazon EC2 instances and load balancers. It can also route traffic to infrastructure outside AWS.\n\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n13.0.6 Example: Route 53 and Amazon CloudFront\n\nCustomer Request: A customer requests data from AnyCompany’s website.\nDNS Resolution: Route 53 resolves the domain name to its corresponding IP address, 192.0.2.0, and sends this information back to the customer.\nEdge Location: The customer’s request is sent to the nearest edge location via Amazon CloudFront.\nContent Delivery: CloudFront connects to the Application Load Balancer, which directs the request to an Amazon EC2 instance.\n\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n13.0.7 Amazon CloudFront\nAmazon CloudFront is a content delivery network (CDN) that uses edge locations to deliver content as close to users as possible, improving latency and speeding up delivery.\n\n\n13.0.8 Example: Content Delivery in South Africa\nSuppose AnyCompany’s website is hosted in the Cape Town Region, and static web assets, like images and GIFs, are deployed in CloudFront edge locations in Johannesburg. When a user in Johannesburg accesses the website, they receive content from the nearby edge location, ensuring quick delivery. For users in other African cities, such as Nairobi or Lagos, similar edge locations can be used to minimise latency and enhance user experience.",
    "crumbs": [
      "Module 4",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>**Global Networking**</span>"
    ]
  },
  {
    "objectID": "module-5-notes/2-Amazon-EBS.html",
    "href": "module-5-notes/2-Amazon-EBS.html",
    "title": "14  Amazon EBS",
    "section": "",
    "text": "14.0.1 Instance stores and Amazon Elastic Block Store (Amazon EBS)\n\n\n14.0.2 How does memory get stored on AWS?\nWhen using Amazon EC2 to run your business applications, those applications need access to CPU, memory, network, and storage. EC2 instances provide access to all these components. Here, we’ll focus on storage. As applications run, they often require access to block-level storage.\nBlock-level storage is like a place to store files, which are series of bytes stored in blocks on a disk. When a file is updated, only the changed blocks are updated, making it an efficient storage type for applications like databases, enterprise software, or file systems.\nJust as you access block-level storage on your personal computer through your hard drive, EC2 instances also have hard drives, with different types available.\n\n\n14.0.3 Instance stores\nInstance store volumes behave like physical hard drives. When you launch an EC2 instance, depending on the instance type, it might provide local storage called instance store volumes. These volumes are physically attached to the host on which your EC2 instance is running. You can write to it just like a normal hard drive.\nHowever, because these volumes are attached to the underlying physical host, all data written to an instance store volume is deleted if you stop or terminate your EC2 instance. This is because the EC2 instance might start up on a different host when restarted, where the original volume does not exist.\nDue to this temporary nature, instance store volumes are useful for data that can be lost without significant consequences, such as temporary files or scratch data.\nAn instance store provides temporary block-level storage for an Amazon EC2 instance. This storage has the same lifespan as the instance, meaning data is lost when the instance is terminated.\nThe key takeaway is not to write important data to the drives that come with EC2 instances. For persistent storage, Amazon offers a service called Amazon Elastic Block Store (EBS).\nTo review an example of how instance stores work, see the steps beloe:\nStep 1: An Amazon EC2 instance with an attached instance store is running.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nStep 2: The instance is stopped or terminated.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nStep 3: All data on the attached instance store is deleted.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n14.0.4 Amazon EBS and snapshots\nAmazon EBS allows you to create virtual hard drives, called EBS volumes, which you can attach to your EC2 instances. These volumes are separate from the local instance store volumes and are not tied directly to the host running your EC2 instance. This means that data written to an EBS volume can persist between stops and starts of an EC2 instance.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nEBS volumes come in various sizes and types. You define the size, type, and configuration of the volume you need, provision it, and attach it to your EC2 instance. Your application can then write to this volume, and the data will remain even if the EC2 instance is stopped and restarted.\nFor data protection, EBS allows you to take incremental backups called snapshots. An EBS snapshot is an incremental backup, meaning only the blocks of data that have changed since the most recent snapshot are saved. This is different from full backups, where all data in a storage volume is copied each time a backup occurs.\n\n\n14.0.5 Amazon EBS snapshots\n\n\nIncremental backups of EBS volumes with Amazon EBS snapshots. On Day 1, two volumes are backed up. Day 2 adds one new volume and the new volume is backed up. Day 3 adds two more volumes for a total of five volumes. Only the two new volumes are backed up.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nTaking regular snapshots of your EBS volumes is crucial to ensure data safety. If a drive becomes corrupted, you can restore data from a snapshot. This provides a reliable backup mechanism, ensuring data persistence and integrity.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>**Amazon EBS**</span>"
    ]
  },
  {
    "objectID": "module-5-notes/3-Amazon-S3.html",
    "href": "module-5-notes/3-Amazon-S3.html",
    "title": "15  Amazon Simple Storage Service (Amazon S3)",
    "section": "",
    "text": "15.1 Understanding object storage\nObject storage is a data storage architecture ideal for storing, archiving, backing up, and managing high volumes of static, unstructured data, such as emails, videos, photos, web pages, audio files, sensor data, and other media and web content.\nObjects in object storage are discrete units of data stored in a structurally flat environment. Unlike file-based systems with folders and directories, each object is a self-contained repository that includes the data, metadata (descriptive information about the object), and a unique ID number for locating and accessing the object.\nObject storage allows for unlimited scaling and improved data resiliency and disaster recovery by aggregating object storage devices into larger pools and distributing these pools across locations. Objects are often stored on cloud servers and accessed via Application Programming Interfaces (APIs), primarily HTTP-based RESTful APIs.\nSource: AWS Cloud Practitioner Essentials\nIn object storage, each object consists of data, metadata, and a key:\nData is stored as objects in buckets. For example, a file on your hard drive is an object, and the file directory is the bucket. The maximum object size for upload is 5 terabytes. Objects can be versioned to protect against accidental deletion, and permissions can be set to control access.\nAmazon S3 provides multiple storage tiers for different use cases, such as frequently accessed data versus long-term archival.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Amazon Simple Storage Service (Amazon S3)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/3-Amazon-S3.html#understanding-object-storage",
    "href": "module-5-notes/3-Amazon-S3.html#understanding-object-storage",
    "title": "15  Amazon Simple Storage Service (Amazon S3)",
    "section": "",
    "text": "Data: This can be an image, video, text document, or any other file type.\nMetadata: Contains information about the data, such as usage, size, and other relevant details.\nKey: A unique identifier for the object.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Amazon Simple Storage Service (Amazon S3)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/3-Amazon-S3.html#amazon-s3",
    "href": "module-5-notes/3-Amazon-S3.html#amazon-s3",
    "title": "15  Amazon Simple Storage Service (Amazon S3)",
    "section": "15.2 Amazon S3",
    "text": "15.2 Amazon S3\nAmazon Simple Storage Service (Amazon S3) is an object storage service offering industry-leading scalability, data availability, security, and performance. It enables customers of all sizes and industries to store, manage, analyze, and protect any amount of data for virtually any use case, such as data lakes, cloud-native applications, and mobile apps. With cost-effective storage classes and easy-to-use management features, you can optimize costs, organize and analyze data, and configure fine-tuned access controls to meet specific business and compliance requirements.\n\n15.2.1 Benefits\nScalability: Amazon S3 can store virtually any amount of data, scaling up to exabytes with unmatched performance. It is fully elastic, automatically growing and shrinking as you add and remove data. There’s no need to provision storage, and you pay only for what you use.\nDurability and availability: Amazon S3 provides the most durable storage in the cloud with industry-leading availability. Its unique architecture is designed to provide 99.999999999% (11 nines) data durability and 99.99% availability by default, supported by the strongest SLAs in the cloud.\nSecurity and data protection: Amazon S3 ensures data security with unmatched protection, compliance, and access control capabilities. Data is secure, private, and encrypted by default, with extensive auditing capabilities to monitor access requests to your S3 resources.\nLowest price and highest performance: For every task, Amazon S3 provides a variety of storage classes with the best price-performance ratios. Massive volumes of data can be cost-effectively stored, regardless of how often, seldom, or infrequently they are used, thanks to automated data lifecycle management. Throughput, latency, resilience, and flexibility offered by S3 guarantee that performance is never constrained by storage.\n\n\n15.2.2 How it works\n\n\nSource: Amazon S3 \n\nAmazon S3 stores data as objects within buckets. An object consists of a file and any metadata that describes the file. A bucket is a container for objects. To store data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region. Then, you upload your data to that bucket as objects in Amazon S3. Each object has a key (or key name), which is the unique identifier for the object within the bucket.\nAmazon S3 provides features that you can configure to support your specific use case:\n\nVersioning: S3 Versioning allows you to keep multiple versions of an object in the same bucket. This feature enables you to restore objects that are accidentally deleted or overwritten.\nAccess controls: Buckets and the objects in them are private and can only be accessed with explicitly granted permissions. You can use bucket policies, AWS Identity and Access Management (IAM) policies, S3 Access Points, and access control lists (ACLs) to manage access.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Amazon Simple Storage Service (Amazon S3)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/3-Amazon-S3.html#amazon-s3-storage-classes",
    "href": "module-5-notes/3-Amazon-S3.html#amazon-s3-storage-classes",
    "title": "15  Amazon Simple Storage Service (Amazon S3)",
    "section": "15.3 Amazon S3 storage classes",
    "text": "15.3 Amazon S3 storage classes\nAmazon S3 offers a variety of storage classes to fit different business and cost needs. When selecting a storage class, consider how often you plan to retrieve your data and how available you need it to be. Here is an overview of the different storage classes, along with use case examples for each.\n\n15.3.1 Amazon S3 standard\nDescription: Designed for frequently accessed data, this storage class stores data in at least three Availability Zones. It provides high availability and durability, making it suitable for a wide range of use cases.\nUse case examples:\n\nWebsites: Hosting static assets such as images, CSS, and JavaScript files that are frequently accessed by users.\nContent distribution: Storing multimedia files for streaming services or content delivery networks (CDNs).\nData analytics: Storing datasets that need to be quickly accessed and processed for analysis.\n\n\n\n15.3.2 Amazon S3 standard-infrequent access (s3 standard-IA)\nDescription: Ideal for data that is infrequently accessed but needs high availability when required. It offers a lower storage price and higher retrieval price compared to S3 Standard, while still storing data in at least three Availability Zones.\nUse case examples:\n\nBackup storage: Storing backups of application data that are not accessed frequently but need to be available when needed.\nDisaster recovery: Keeping disaster recovery copies of critical business data that must be quickly accessible in case of an emergency.\n\n\n\n15.3.3 Amazon S3 one zone-infrequent access (s3 one zone-IA)\nDescription: Stores data in a single Availability Zone, offering lower storage costs. It is suitable for data that can be easily reproduced in the event of an Availability Zone failure.\nUse case examples:\n\nNon-critical data: Storing data that can be recreated or is not critical to business operations, such as temporary logs or redundant copies of data.\nCost-sensitive backups: Storing backup copies where cost savings are prioritized over redundancy.\n\n\n\n15.3.4 Amazon S3 intelligent-tiering\nDescription: Ideal for data with unknown or changing access patterns. This storage class automatically moves objects between frequent and infrequent access tiers based on usage, with a small monthly monitoring and automation fee per object.\nUse case examples:\n\nDynamic data: Storing data where access patterns are unpredictable, such as data from IoT devices or logs that might be accessed irregularly.\nCost optimization: Automatically optimizing costs by moving data to the most cost-effective storage tier without manual intervention.\n\n\n\n15.3.5 Amazon S3 glacier instant retrieval\nDescription: Suitable for archived data that requires immediate access, with retrieval times within milliseconds.\nUse case examples:\n\nMedical records: Storing medical records that need to be archived but must be quickly accessible for compliance or medical emergencies.\nLegal documents: Archiving legal documents that might be needed immediately for legal proceedings or audits.\n\n\n\n15.3.6 Amazon S3 glacier flexible retrieval\nDescription: Low-cost storage for data archiving, with retrieval times ranging from minutes to hours. It is suitable for storing archived customer records or older media files.\nUse case examples:\n\nCustomer records: Archiving old customer records that are rarely accessed but must be retained for regulatory compliance.\nMedia archives: Storing older video footage or historical data that can afford longer retrieval times.\n\n\n\n15.3.7 Amazon S3 glacier deep archive\nDescription: The lowest-cost storage class for long-term data retention, with retrieval times from 12 to 48 hours. It is ideal for data accessed once or twice a year.\nUse case examples:\n\nCompliance data: Storing data required for long-term retention, such as financial records or audit logs, which are rarely accessed.\nHistorical archives: Archiving historical documents, research data, or records that are only needed occasionally.\n\n\n\n15.3.8 Amazon S3 outposts\nDescription: Provides object storage on-premises with AWS Outposts, designed for local data residency requirements and demanding performance needs.\nUse case examples:\n\nLocal data residency: Storing sensitive data that must remain within a specific geographic location due to regulatory or compliance requirements.\nHigh-performance applications: Using for applications that require low-latency access to data stored on-premises, such as manufacturing systems or edge computing environments.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Amazon Simple Storage Service (Amazon S3)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/3-Amazon-S3.html#comparing-amazon-ebs-and-amazon-s3",
    "href": "module-5-notes/3-Amazon-S3.html#comparing-amazon-ebs-and-amazon-s3",
    "title": "15  Amazon Simple Storage Service (Amazon S3)",
    "section": "15.4 Comparing Amazon EBS and Amazon S3",
    "text": "15.4 Comparing Amazon EBS and Amazon S3\n\n15.4.1 Amazon S3 benefits\nAmazon Simple Storage Service (Amazon S3) is AWS’s object storage solution. If you’ve ever used services like Google Drive or Dropbox, you’ll understand the basic concept of S3. At first glance, S3 seems like a simple place to store files, photos, videos, and other documents. However, S3 offers far more than basic object storage.\nS3 provides scalable solutions, meaning it can grow or shrink to meet your needs in a cost-effective manner as your project size changes. It also helps you manage data effectively, allowing you to control who accesses your content. S3 offers robust data protection against various threats, replicates your data for increased durability, and provides different storage classes to help you save money.\n\n\n15.4.2 Amazon S3 use cases\n\nBackup and restore: Have you ever accidentally deleted something important? S3’s backup and restore capabilities ensure users don’t lose data through versioning and deletion protection. Versioning saves a new version of a file every time it’s updated, and deletion protection ensures the user has the right permissions before deleting a file.\nDisaster recovery: What would a company do during an unexpected power outage or if their on-premises data center suddenly crashed? S3 data is protected in Amazon-managed data centers, the same ones Amazon uses to host their world-famous shopping website. By using S3, users get a second storage option without having to pay for the rent and utilities of a physical site.\nArchiving: Some businesses need to store financial, medical, or other data mandated by industry standards. AWS allows users to archive this type of data with S3 Glacier, one of the many S3 storage classes. S3 Glacier is a cost-effective solution for archiving and is one of the best in the market.\n\n\n\n15.4.3 Amazon EBS benefits\nAmazon Elastic Block Store (Amazon EBS) is AWS’s block storage service. EBS is different from S3 in that it provides a storage volume directly connected to EC2 (Elastic Cloud Compute) instances. EBS allows you to store files directly on an EC2 instance, enabling quick and cost-effective access to your files. Think of EBS as “EC2 storage.”\nYou can customize your EBS volumes to suit the workload. For example, if you need greater throughput, you could choose a Throughput Optimized HDD EBS volume. If you have no specific needs, an EBS General Purpose SSD might suffice. For high-performance needs, an EBS Provisioned IOPS SSD volume is suitable.\nJust remember that EBS works with EC2 in a similar way to how your hard drive works with your computer. EBS lets you save files locally to an EC2 instance, allowing your EC2 to perform powerful tasks that would otherwise be impossible. Let’s look at a couple of examples.\n\n\n15.4.4 Amazon EBS use cases\n\nDatabase performance: Many companies look for cheaper ways to run their databases. Amazon EBS provides both relational and NoSQL databases with scalable solutions that have low-latency performance. Slack, the messaging app, uses EBS to increase database performance to better serve customers worldwide.\nInstance backup: Another use case for EBS involves backing up your instances. Because EBS is an AWS-native solution, the backups you create in EBS can easily be uploaded to S3 for convenient and cost-effective storage. This ensures you can always recover to a certain point in time if needed.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Amazon Simple Storage Service (Amazon S3)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/4-Amazon-EFS.html",
    "href": "module-5-notes/4-Amazon-EFS.html",
    "title": "16  Amazon Elastic File System (Amazon EFS)",
    "section": "",
    "text": "16.1 Understanding file storage\nIn file storage, multiple clients (such as users, applications, and servers) can access data stored in shared file folders. A storage server uses block storage with a local file system to organize files, and clients access data through file paths. Compared to block storage and object storage, file storage is ideal when many services and resources need to access the same data simultaneously.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Amazon Elastic File System (Amazon EFS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/4-Amazon-EFS.html#amazon-efs-a-managed-file-system",
    "href": "module-5-notes/4-Amazon-EFS.html#amazon-efs-a-managed-file-system",
    "title": "16  Amazon Elastic File System (Amazon EFS)",
    "section": "16.2 Amazon EFS: A managed file system",
    "text": "16.2 Amazon EFS: A managed file system\nAmazon Elastic File System (Amazon EFS) is a scalable file system used with AWS Cloud services and on-premises resources. As you add and remove files, Amazon EFS automatically grows and shrinks without disrupting applications. It can scale on demand to petabytes, allowing multiple instances to access the data simultaneously. With EFS, you don’t need to worry about buying hardware or managing the file system’s operational aspects. AWS handles scaling and replication, ensuring seamless performance.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Amazon Elastic File System (Amazon EFS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/4-Amazon-EFS.html#comparing-amazon-ebs-and-amazon-efs",
    "href": "module-5-notes/4-Amazon-EFS.html#comparing-amazon-ebs-and-amazon-efs",
    "title": "16  Amazon Elastic File System (Amazon EFS)",
    "section": "16.3 Comparing Amazon EBS and Amazon EFS",
    "text": "16.3 Comparing Amazon EBS and Amazon EFS\nYou might wonder about the difference between Amazon EBS and Amazon EFS, as both allow you to store files accessible from EC2 instances. The distinction is straightforward:\n\nAmazon EBS (Elastic Block Store): EBS volumes attach to EC2 instances and are an Availability Zone-level resource. Both the EC2 instance and the EBS volume must be in the same Availability Zone. EBS acts like a hard drive where you can save files, run databases, or store applications. However, if you provision a two-terabyte EBS volume and fill it up, it doesn’t automatically scale.\nAmazon EFS (Elastic File System): EFS, on the other hand, is a regional service that stores data across multiple Availability Zones. Any EC2 instance in the Region can write to the EFS file system, and it automatically scales as you write more data. EFS allows multiple instances to read and write from it simultaneously, functioning as a true file system for Linux. Additionally, on-premises servers can access Amazon EFS using AWS Direct Connect.\n\nAmazon EFS offers the advantage of being a regional service, enabling concurrent data access from all Availability Zones in the Region where the file system is located. This ensures high availability and durability, making it suitable for applications that require scalable and reliable shared file storage.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Amazon Elastic File System (Amazon EFS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/5-Amazon-RDS.html",
    "href": "module-5-notes/5-Amazon-RDS.html",
    "title": "17  Amazon Relational Database Service (Amazon RDS)",
    "section": "",
    "text": "17.1 Relational databases\nTo address this complexity, it’s best to use a relational database management system (RDBMS). An RDBMS stores data in a structured way that relates different pieces of data to one another. For example, customer information can be stored in a customer table, while physical addresses can be stored in an address table. These tables can be related via a common attribute, such as a customer ID, allowing for efficient querying and data management.\nSQL (Structured Query Language) is commonly used to query relational databases. Popular database systems supported by AWS include MySQL, PostgreSQL, Oracle, and Microsoft SQL Server. Many businesses run these databases on-premises, but they can be moved to the cloud for greater flexibility and scalability.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Amazon Relational Database Service (Amazon RDS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/5-Amazon-RDS.html#moving-databases-to-the-cloud",
    "href": "module-5-notes/5-Amazon-RDS.html#moving-databases-to-the-cloud",
    "title": "17  Amazon Relational Database Service (Amazon RDS)",
    "section": "17.2 Moving databases to the cloud",
    "text": "17.2 Moving databases to the cloud\nOne method for moving on-premises databases to the cloud is called Lift-and-Shift, which involves migrating your database to run on Amazon EC2. This allows you to maintain control over variables like the operating system, memory, CPU, and storage capacity, similar to your on-premises environment. Standard migration practices or tools like the Database Migration Service can facilitate this process.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Amazon Relational Database Service (Amazon RDS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/5-Amazon-RDS.html#amazon-rds-a-managed-service",
    "href": "module-5-notes/5-Amazon-RDS.html#amazon-rds-a-managed-service",
    "title": "17  Amazon Relational Database Service (Amazon RDS)",
    "section": "17.3 Amazon RDS: A managed service",
    "text": "17.3 Amazon RDS: A managed service\nA more managed option for running databases in the cloud is Amazon Relational Database Service (Amazon RDS). Amazon RDS supports major database engines and offers numerous benefits, including automated patching, backups, redundancy, failover, and disaster recovery. These features reduce the administrative burden on database administrators, allowing them to focus on solving business problems rather than maintaining databases.\nIn a relational database, data is stored in a way that relates it to other pieces of data, making it easy to understand, consistent, and scalable. For example, a coffee shop owner can write an SQL query to identify all customers whose most frequently purchased drink is a medium latte.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Amazon Relational Database Service (Amazon RDS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/5-Amazon-RDS.html#example-of-data-in-a-relational-database",
    "href": "module-5-notes/5-Amazon-RDS.html#example-of-data-in-a-relational-database",
    "title": "17  Amazon Relational Database Service (Amazon RDS)",
    "section": "17.4 Example of data in a relational database",
    "text": "17.4 Example of data in a relational database\nConsider a relational database for an online bookstore. It might include the following tables:\n\nCustomers: Stores customer information like name, email, and contact details.\nBooks: Stores details about books like title, author, ISBN, and price.\nOrders: Stores order information, linking customers to the books they purchased, including order date and quantity.\n\nThese tables are related through keys, allowing efficient data retrieval and management.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Amazon Relational Database Service (Amazon RDS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/5-Amazon-RDS.html#amazon-rds",
    "href": "module-5-notes/5-Amazon-RDS.html#amazon-rds",
    "title": "17  Amazon Relational Database Service (Amazon RDS)",
    "section": "17.5 Amazon RDS",
    "text": "17.5 Amazon RDS\nAmazon Relational Database Service (Amazon RDS) enables you to run relational databases in the AWS Cloud. It automates tasks such as hardware provisioning, database setup, patching, and backups, freeing up time to focus on using data to innovate your applications. Amazon RDS can be integrated with other services, such as AWS Lambda, to query your database from a serverless application.\nAmazon RDS provides various security options, including encryption at rest (protecting data while it is stored) and encryption in transit (protecting data while it is being sent and received).\n\n17.5.1 Amazon RDS database engines\nAmazon RDS supports six database engines, each optimized for different aspects like memory, performance, or input/output (I/O). The supported database engines are:\n\nAmazon Aurora\nPostgreSQL\nMySQL\nMariaDB\nOracle Database\nMicrosoft SQL Server\n\nBy using Amazon RDS, you can leverage the power of these database engines with the added benefits of AWS’s managed services, ensuring efficient and secure data management.\n\n\n17.5.2 Amazon Aurora\nAmazon Aurora is an enterprise-class relational database that is compatible with MySQL and PostgreSQL. It offers significant performance improvements, being up to five times faster than standard MySQL databases and up to three times faster than standard PostgreSQL databases.\n\n\n17.5.3 Key benefits of Amazon Aurora\n\nPerformance:\n\nAmazon Aurora is designed to deliver high performance with low latency. It achieves this by optimizing the database engine and reducing unnecessary input/output (I/O) operations. This results in faster query processing and improved overall performance compared to traditional MySQL and PostgreSQL databases.\n\nCost efficiency:\n\nAurora helps reduce database costs by minimizing unnecessary I/O operations. This means you get more efficient use of your database resources, leading to cost savings. Additionally, Aurora’s pay-as-you-go pricing model ensures that you only pay for the resources you actually use.\n\nHigh availability:\n\nAurora is built for high availability and reliability. It replicates six copies of your data across three Availability Zones. This multi-AZ replication ensures that your data is always available, even in the event of an infrastructure failure in one of the Availability Zones. Additionally, Aurora continuously backs up your data to Amazon S3, providing an extra layer of data protection.\n\nScalability:\n\nAmazon Aurora can automatically scale storage as needed, up to 128 terabytes per database instance. This means you don’t have to worry about running out of storage space or manually provisioning additional storage. Aurora also supports read replicas, which allows you to distribute read traffic and scale read operations independently from write operations.\n\nSecurity:\n\nAurora offers robust security features, including encryption at rest and in transit, network isolation using Amazon Virtual Private Cloud (VPC), and fine-grained access control with AWS Identity and Access Management (IAM). These features help ensure that your data is secure and compliant with industry standards.\n\n\n\n\n17.5.4 Use cases for Amazon Aurora\n\nHigh-performance applications: If your applications require high transaction throughput and low latency, Aurora is an excellent choice. Examples include e-commerce platforms, financial applications, and online gaming.\nScalable web applications: Aurora’s ability to automatically scale storage and support read replicas makes it ideal for web applications that experience variable traffic patterns and need to handle large amounts of read traffic.\nData warehousing and analytics: Aurora’s high performance and scalability make it suitable for data warehousing and analytics workloads, where fast query processing and the ability to handle large datasets are critical.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Amazon Relational Database Service (Amazon RDS)</span>"
    ]
  },
  {
    "objectID": "module-5-notes/6-Amazon-DynamoDB.html",
    "href": "module-5-notes/6-Amazon-DynamoDB.html",
    "title": "18  Amazon DynamoDB",
    "section": "",
    "text": "19 Amazon DynamoDB",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Amazon DynamoDB</span>"
    ]
  },
  {
    "objectID": "module-5-notes/6-Amazon-DynamoDB.html#introduction-to-non-relational-nosql-databases",
    "href": "module-5-notes/6-Amazon-DynamoDB.html#introduction-to-non-relational-nosql-databases",
    "title": "18  Amazon DynamoDB",
    "section": "19.1 Introduction to Non-Relational NoSQL Databases",
    "text": "19.1 Introduction to Non-Relational NoSQL Databases\nNon-relational databases, also known as NoSQL databases, provide an alternative to the traditional relational database model by using different structures to organise data. Unlike relational databases that use rows and columns, non-relational databases often utilise key-value pairs. This flexible schema allows for more dynamic and scalable data management, making them suitable for various use cases where performance and flexibility are critical.\n\n19.1.1 Non-Relational Databases\nIn a non-relational database, data is stored in tables. Each table contains items (keys), and each item has attributes (values). Attributes can be thought of as different features of your data. Key-value databases are a common type of non-relational database. They allow you to add or remove attributes from items in the table at any time, and not every item needs to have the same attributes.\n\n\n19.1.2 Example of Non-Relational Database\nNon-relational databases are particularly useful when dealing with datasets that are less rigid and require high access rates. Traditional SQL databases, with their fixed schemas and performance constraints under stress, might not be the best fit for such scenarios. This is where non-relational databases, such as DynamoDB, excel.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Amazon DynamoDB</span>"
    ]
  },
  {
    "objectID": "module-5-notes/6-Amazon-DynamoDB.html#amazon-dynamodb-1",
    "href": "module-5-notes/6-Amazon-DynamoDB.html#amazon-dynamodb-1",
    "title": "18  Amazon DynamoDB",
    "section": "19.2 Amazon DynamoDB",
    "text": "19.2 Amazon DynamoDB\nAmazon DynamoDB is a fully managed, serverless, key-value database service that offers single-digit millisecond performance at any scale. It provides a flexible and scalable solution for applications requiring consistent performance and high throughput.\n\n19.2.1 Key Features of DynamoDB\n\nServerless Architecture: DynamoDB is a serverless database, meaning you do not need to manage the underlying infrastructure. Amazon handles all the server provisioning, patching, and maintenance, allowing you to focus on your application development.\nScalability: DynamoDB automatically scales to accommodate changes in data volume and throughput, ensuring consistent performance regardless of database size. This automatic scaling makes it ideal for applications with variable or unpredictable workloads.\nHigh Availability and Durability: DynamoDB stores data redundantly across multiple availability zones, mirroring data across multiple drives. This redundancy ensures high availability and durability, reducing the operational burden of managing a highly available database.\nMillisecond Performance: DynamoDB provides millisecond response times, which is crucial for applications with millions of users and high-performance requirements. This quick response time enhances user experience and application reliability.\nFlexible Schema: Unlike traditional relational databases, DynamoDB allows you to add or remove attributes from items at any time. This flexibility supports datasets with varying attributes, making it a suitable choice for diverse and evolving data models.\n\n\n\n19.2.2 Querying DynamoDB\nDynamoDB does not use SQL for querying data. Instead, it relies on a simpler query model based on a subset of attributes designated as keys. This model supports efficient queries focusing on collections of items within a single table rather than complex queries spanning multiple tables.\n\n\n19.2.3 Use Cases and Performance\nDynamoDB is purpose-built for specific use cases requiring high scalability and performance. For example, during Prime Day in 2019, DynamoDB handled 7.11 trillion API calls over 48 hours, peaking at 45.4 million requests per second. This level of scalability and performance is achieved without the need for manual database management, showcasing the power and efficiency of DynamoDB.\n\n\n19.2.4 Comparing Amazon RDS and Amazon DynamoDB\n\n\n19.2.5 Amazon RDS\n\nAutomatic high availability; recovery provided\nCustomer ownership of data\nCustomer ownership of schema\nCustomer control of network\n\n\n\n19.2.6 Amazon DynamoDB\n\nKey-value\nMassive throughput capabilities\nPB size potential\nGranular API access",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Amazon DynamoDB</span>"
    ]
  },
  {
    "objectID": "module-5-notes/7-Amazon-Redshift.html",
    "href": "module-5-notes/7-Amazon-Redshift.html",
    "title": "19  Amazon Redshift",
    "section": "",
    "text": "Imagine a large retail company that collects data from various sources, such as sales transactions, customer feedback, inventory levels, and website interactions. This company needs to analyse all this data to understand customer behaviour, optimise inventory, and improve sales strategies. While they’ve used different storage solutions tailored to specific needs, they now face the challenge of quickly processing and analysing this massive volume of data to gain actionable insights. This is where Amazon Redshift can be useful.\nAmazon Redshift\nAmazon Redshift is a data-warehousing service that you can use for big data analytics. It offers the ability to collect data from many sources and helps you understand relationships and trends across your data. Think of it as a supercharged storage system that can handle vast amounts of data, making it easier for companies to find the information they need. Redshift takes care of the heavy lifting, like setting up and managing the infrastructure, so businesses can focus on analysing their data rather than worrying about technical details.\nThe key benefit of Amazon Redshift is its speed and efficiency. It can handle huge data sets and run complex queries much faster than traditional databases. This is crucial for businesses that need to make sense of large amounts of information quickly. By using Redshift, companies can get valuable insights from their data without waiting too long, helping them make better decisions and stay competitive.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>**Amazon Redshift**</span>"
    ]
  },
  {
    "objectID": "module-5-notes/8-AWS-DMS.html",
    "href": "module-5-notes/8-AWS-DMS.html",
    "title": "20  AWS DMS",
    "section": "",
    "text": "20.0.1 AWS Database Migration Service (AWS DMS)\nIf you have a database that’s on-premises or already in the cloud, AWS offers a service called Amazon Database Migration Service (DMS). AWS DMS helps customers migrate existing databases onto AWS in a secure and easy manner. It enables you to migrate data between a source and a target database with minimal downtime, keeping the source database fully operational during the migration. Additionally, the source and target databases don’t have to be of the same type.\n\n\n20.0.2 Homogeneous Migrations (Same Type Databases)\nWhen both the source and target databases are of the same type, the migration is known as homogeneous. Examples include:\n\nMySQL to Amazon RDS for MySQL\nMicrosoft SQL Server to Amazon RDS for SQL Server\nOracle to Amazon RDS for Oracle\n\nThis process is straightforward since the schema structures, data types, and database code are compatible between the source and target databases.\n\n\n20.0.3 Heterogeneous Migrations (Different Type Databases)\nWhen the source and target databases are of different types, the migration is known as heterogeneous. This process involves two steps:\n\nSchema Conversion: Use the AWS Schema Conversion Tool to convert the source schema and database code to match the target database.\nData Migration: Use AWS DMS to migrate the data from the source database to the target database.\n\n\n\n20.0.4 Other Use Cases\nAWS DMS can also be used for various other purposes, including:\n\nDevelopment and Test Migrations: Migrate a copy of your production database to your development or test environments. This allows you to develop and test against production data without affecting production users. This migration can be done once or continuously.\nDatabase Consolidation: Consolidate several databases into one central database.\nContinuous Replication: Perform continuous data replication for disaster recovery or because of geographic separation.\n\nAWS DMS makes database migrations efficient and minimises downtime, ensuring a seamless transition for your applications and users.",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>AWS DMS</span>"
    ]
  },
  {
    "objectID": "module-5-notes/9-Additional_database_service.html",
    "href": "module-5-notes/9-Additional_database_service.html",
    "title": "21  Amazon DocumentDB",
    "section": "",
    "text": "Amazon DocumentDB is a managed document database service that supports MongoDB workloads. It is designed to handle, store, and query JSON data efficiently, making it ideal for applications that work with semi-structured data. DocumentDB provides scalability, high availability, and security, while maintaining compatibility with MongoDB drivers and tools.\n\n21.0.1 Use Cases:\n\nContent management systems\nMobile and web applications\nCatalogues and user profiles\n\n\n\n21.0.2 Amazon Neptune\nAmazon Neptune is a fully managed graph database service that supports both property graph and RDF graph models. It is designed for applications that require highly connected data and complex queries, such as social networking, recommendation engines, and fraud detection.\n\n\n21.0.3 Use Cases:\n\nSocial networking applications\nKnowledge graphs\nFraud detection systems\n\n\n\n21.0.4 Amazon Quantum Ledger Database (Amazon QLDB)\nAmazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log. This service is ideal for applications that require an authoritative data source and a complete and verifiable history of changes.\n\n\n21.0.5 Use Cases:\n\nSupply chain tracking\nFinancial transaction systems\nRegistries and record-keeping\n\n\n\n21.0.6 Amazon Managed Blockchain\nAmazon Managed Blockchain is a fully managed service that allows you to create and manage scalable blockchain networks using popular open-source frameworks like Hyperledger Fabric and Ethereum. This service simplifies the setup and management of blockchain networks, making it easier to build applications where multiple parties can execute transactions without a trusted central authority.\n\n\n21.0.7 Use Cases:\n\nSupply chain transparency\nFinancial services\nDecentralised applications\n\n\n\n21.0.8 Amazon ElastiCache\nAmazon ElastiCache is a fully managed in-memory caching service that supports Redis and Memcached. It is designed to improve the performance of web applications by retrieving data from high throughput and low latency in-memory caches, rather than relying entirely on slower disk-based databases.\n\n\n21.0.9 Use Cases:\n\nReal-time analytics\nGaming leaderboards\nSession storage\n\n\n\n21.0.10 Amazon DynamoDB Accelerator (DAX)\nAmazon DynamoDB Accelerator (DAX) is a fully managed, highly available in-memory cache for DynamoDB that delivers up to a 10x performance improvement – from milliseconds to microseconds – even at millions of requests per second. DAX handles the heavy lifting of cache management, freeing developers from the complexity of designing, deploying, and managing a distributed cache.\n\n\n21.0.11 Use Cases:\n\nHigh-traffic web applications\nE-commerce applications\nGaming applications",
    "crumbs": [
      "Module 5",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Amazon DocumentDB</span>"
    ]
  },
  {
    "objectID": "module-6-notes/1-Introduction.html",
    "href": "module-6-notes/1-Introduction.html",
    "title": "Module 6",
    "section": "",
    "text": "Introduction\nIn this module, you will learn how to:\nExplain the benefits of the shared responsibility model. Describe multi-factor authentication (MFA). Differentiate between the AWS Identity and Access Management (IAM) security levels. Explain the main benefits of AWS Organizations. Describe security policies at a basic level. Summarize the benefits of compliance with AWS. Explain additional AWS security services at a basic level.",
    "crumbs": [
      "Module 6"
    ]
  },
  {
    "objectID": "module-6-notes/2-AWS-sahred.html",
    "href": "module-6-notes/2-AWS-sahred.html",
    "title": "23  AWS shared responsibility model",
    "section": "",
    "text": "23.0.1 Understanding Security Responsibilities in AWS\nAWS provides a variety of resources and services, including on-premises databases, networking, and EC2 instances. However, a key question often arises: when maintaining and running these sophisticated services, who is responsible for their security?\nThe response is that AWS and the customer share this obligation. AWS environments are managed as interconnected components rather than as isolated entities. AWS takes care of some aspects, while you, the customer, are responsible for others. This notion is referred to as the shared responsibility model.The shared responsibility model splits into two areas: customer responsibilities, often called “security in the cloud,” and AWS responsibilities, known as “security of the cloud.” You can think of this model as similar to the relationship between a homeowner and a homebuilder. The builder (AWS) is responsible for constructing the house and ensuring its structural integrity. The homeowner (the customer) is responsible for safeguarding the house’s contents, including locking the doors and windows.\n\n\nSource: Shared Responsibility Model\n\nCustomers: Security in the Cloud\nAs a customer, you are responsible for the security of everything that you create and store in the AWS Cloud. When using AWS services, you have complete control over your content. This means you are responsible for managing security requirements for your content, including which content you choose to store on AWS, which AWS services you use, and who has access to that content. You also control how access rights are granted, managed, and revoked.\nThe security steps you take will depend on factors such as the services you use, the complexity of your systems, and your company’s specific operational and security needs. These steps include:\n\nSelecting, configuring, and patching the operating systems that will run on Amazon EC2 instances.\nConfiguring security groups.\nManaging user accounts.\n\nAWS: Security of the Cloud\nAWS is responsible for the security of the cloud. This means AWS operates, manages, and controls the components at all layers of infrastructure. This includes areas such as the host operating system, the virtualisation layer, and even the physical security of the data centres from which services operate.\nAWS is responsible for protecting the global infrastructure that runs all the services offered in the AWS Cloud. This infrastructure includes AWS Regions, Availability Zones, and edge locations. AWS manages the security of the cloud, specifically the physical infrastructure that hosts your resources, which includes:\n\nPhysical security of data centres\nHardware and software infrastructure\nNetwork infrastructure\nVirtualisation infrastructure\n\nAlthough you cannot visit AWS data centres to see this protection firsthand, AWS provides several reports from third-party auditors. These auditors have verified its compliance with various computer security standards and regulations.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>**AWS shared responsibility model**</span>"
    ]
  },
  {
    "objectID": "module-6-notes/3-AWS-permission.html",
    "href": "module-6-notes/3-AWS-permission.html",
    "title": "24  User Permissions and Access",
    "section": "",
    "text": "24.0.1 Understanding User Permissions and Access in AWS\nAWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. IAM provides the flexibility to configure access based on your company’s specific operational and security needs. This guide will cover the key features of IAM, including IAM users, groups, roles, policies, and multi-factor authentication (MFA), along with best practices for each.\n\n\n24.0.2 AWS Account Root User\nWhen you first create an AWS account, you start with an identity known as the root user. The root user is accessed using the email address and password you used to create the account. Think of the root user as the owner of a coffee shop who has complete control over everything.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nBest Practice: Avoid using the root user for everyday tasks. Instead, use the root user to create an initial IAM user with permissions to create other users. Continue to create individual IAM users for regular tasks and reserve the root user for tasks that require its unique privileges, such as changing the root user email address or updating your AWS support plan.\n\n\n24.0.3 IAM Users\nAn IAM user is an identity you create in AWS, representing a person or application that interacts with AWS services. An IAM user has a name and credentials. By default, new IAM users have no permissions, so you must grant them the necessary permissions to perform specific actions, such as launching an EC2 instance or creating an S3 bucket.\nBest Practice: Create individual IAM users for each person who needs access to AWS, even if they require the same level of access. This ensures each user has unique security credentials, enhancing security.\n\n\n24.0.4 IAM Policies\nIAM policies are documents that define permissions for AWS services and resources. Policies allow you to customise access levels for different users, such as granting access to all S3 buckets or just a specific bucket.\nBest Practice: Follow the principle of least privilege, granting users only the permissions they need to perform their tasks. For example, if an employee needs access to a specific bucket, specify that bucket in the IAM policy rather than granting access to all buckets.\n\n\n24.0.5 IAM Groups\nAn IAM group is a collection of IAM users. Assigning a policy to a group grants the permissions specified in the policy to all users in the group. For example, a coffee shop owner could create a “Cashiers” group and assign permissions to that group, making it easy to manage access for all cashiers.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nBest Practice: Use IAM groups to manage permissions efficiently. If an employee changes roles, move them to a different group to adjust their permissions accordingly.\n\n\n24.0.6 IAM Roles\nIAM roles provide temporary access to permissions for users, applications, or services. An IAM role can be assumed to gain the permissions associated with that role, similar to a coffee shop employee switching between different tasks throughout the day.\nBest Practice: Use IAM roles for temporary access to services or resources rather than granting long-term permissions. This is particularly useful for users who need to perform various tasks that require different permissions.\n\n\n24.0.7 Multi-Factor Authentication (MFA)\nMFA adds an extra layer of security to your AWS account by requiring additional verification beyond just a password. For example, you might need to enter a code sent to your phone after providing your password.\nBest Practice: Enable MFA for all IAM users to enhance the security of your AWS account.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>User Permissions and Access</span>"
    ]
  },
  {
    "objectID": "module-6-notes/4-AWS-Organisations.html",
    "href": "module-6-notes/4-AWS-Organisations.html",
    "title": "25  AWS Organisations",
    "section": "",
    "text": "25.0.1 Permissions and Access in AWS\nWhen starting with AWS, it usually begins with a single account. However, as your company grows or embarks on its cloud journey, it’s crucial to establish a separation of duties. For instance, you may want your developers to have access to development resources, your accounting staff to access billing information, or different business units to experiment with AWS services without affecting each other.\nManaging multiple accounts can quickly become complicated. You might need to track various accounts with different permissions and responsibilities. To address this, AWS offers a service called AWS Organisations, which simplifies the management of multiple AWS accounts.\n\n\n25.0.2 AWS Organisations\nAWS Organisations allows you to consolidate and manage multiple AWS accounts within a central location. When you create an organisation, AWS Organisations automatically creates a root, which is the parent container for all accounts in your organisation. This service enables you to manage billing, control access, ensure compliance, enhance security, and share resources across your AWS accounts.\nKey Features of AWS Organisations:\n\nCentralised Management: AWS Organisations provides a central location to manage all your AWS accounts. For example, if you have accounts A, B, C, F, and G, you can combine them into one organisation, making it easier to manage and discover additional accounts like D and E.\nConsolidated Billing: With AWS Organisations, you can use the primary account to consolidate and pay for all member accounts. This not only simplifies billing but also offers bulk discounts, saving money.\nService Control Policies (SCPs): SCPs allow you to control the AWS services, resources, and individual API actions that users and roles in each account can access. As the administrator of the primary account, you can specify the maximum permissions for member accounts, ensuring that users only have access to what they need.\nOrganisational Units (OUs): You can group accounts into OUs to manage accounts with similar business or security requirements more efficiently. When you apply a policy to an OU, all accounts within that OU inherit the specified permissions. This feature helps isolate workloads or applications with specific security requirements.\n\nExample: If you have accounts that need to comply with regulatory requirements, you can place them into an OU and attach a policy that blocks access to non-compliant AWS services.\n\n\n\n\n25.0.3 Implementing Best Practices\n\nStart with the Root User: Begin by creating an IAM user with administrative permissions to manage other users and resources. Avoid using the root user for daily tasks to minimise security risks.\nCreate IAM Users: Create individual IAM users for each person needing access to AWS. Assign specific permissions to each user based on their role and responsibilities.\nUtilise IAM Policies: Define permissions through IAM policies, following the principle of least privilege. Ensure users only have access to the resources they need to perform their tasks.\nLeverage IAM Groups: Group users with similar access needs into IAM groups and assign policies at the group level. This simplifies permission management and ensures consistency.\nEmploy IAM Roles: Use IAM roles for temporary access to AWS resources. Roles are ideal for users or applications that need different levels of access at different times.\nEnable Multi-Factor Authentication (MFA): Enhance security by enabling MFA for all IAM users. MFA requires additional verification, providing an extra layer of protection.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AWS Organisations</span>"
    ]
  },
  {
    "objectID": "module-6-notes/5-Compliance.html",
    "href": "module-6-notes/5-Compliance.html",
    "title": "26  Compliance in AWS",
    "section": "",
    "text": "In the context of AWS (Amazon Web Services), compliance refers to following particular standards and regulations that are relevant to the industry you operate in. You will undergo an audit or inspection to ensure that you fulfil these requirements.\n\n26.0.1 Why Compliance Matters\nRegardless of your business type, maintaining compliance is crucial. For example, if you manage software that handles consumer data in the EU, you must comply with the GDPR (General Data Protection Regulation). If you run healthcare applications in the US, you need to meet HIPAA (Health Insurance Portability and Accountability Act) compliance requirements. Compliance ensures that your operations are legal, secure, and trustworthy.\n\n\n26.0.2 AWS and Compliance\nWhen using AWS, compliance involves both AWS’s responsibilities and your own. AWS has built its infrastructure following industry best practices for security and compliance. As an AWS customer, you inherit these best practices. AWS complies with numerous assurance programmes, which means many aspects of compliance are already handled, allowing you to focus on your application and data.\n\n\n26.0.3 Shared Responsibility Model\nThe AWS Shared Responsibility Model outlines the division of security and compliance duties between AWS and you, the customer. AWS is responsible for cloud security, which encompasses physical infrastructure, hardware, and operational software (such as networking and databases). You are responsible for the security in the cloud, which includes the management of your data, user permissions, and configurations. You’re in charge of the architecture, therefore you have to make sure it complies with regulations.\n\n\n26.0.4 Tools for Compliance\nAWS offers various tools to help you maintain compliance:\n\nAWS Artifact: This service provides on-demand access to AWS security and compliance reports and select online agreements. It has two main sections:\n\nAWS Artifact Agreements: Here, you can review, accept, and manage agreements that address specific regulatory needs, such as HIPAA.\nAWS Artifact Reports: This section offers compliance reports from third-party auditors who verify that AWS complies with global, regional, and industry-specific standards. These reports can be shared with your auditors to demonstrate compliance.\n\nAWS Compliance Center: This is a resource hub where you can find compliance-related information, including customer stories, whitepapers, and documentation on AWS risk and compliance. It also includes an auditor learning path to help your team understand how to use AWS to meet compliance requirements.\nRegion Selection: The AWS Region you choose to operate in can help meet compliance needs, especially if regulations require data to remain within specific geographic boundaries. AWS ensures that data does not automatically replicate across regions, helping you adhere to local data storage laws.\n\n\n\n26.0.5 Data Ownership and Protection\nYou own your data in AWS. This means you have full control over how your data is stored, encrypted, and accessed. AWS provides various encryption mechanisms across its services, often configurable with just a setting change. This flexibility allows you to meet specific data protection standards required by your industry.\n\n\n26.0.6 Documentation and Audits\nAWS provides extensive documentation and whitepapers to support your compliance efforts. For instance, the AWS Risk and Security Whitepaper outlines best practices for security and compliance in AWS. You can request documentation proving that AWS follows these practices, which is crucial during audits.\n\n\n26.0.7 Conclusion\nCompliance in AWS is a shared responsibility where AWS manages the security of the cloud infrastructure, and you manage the security of your data and applications within the cloud. By utilising tools like AWS Artifact and the AWS Compliance Center, you can streamline your compliance efforts, ensuring your operations meet industry standards and regulations. This collaborative approach helps you maintain a secure, compliant, and trustworthy environment for your business operations.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Compliance in AWS</span>"
    ]
  },
  {
    "objectID": "module-6-notes/6-Denial-of-Service-Attacks.html",
    "href": "module-6-notes/6-Denial-of-Service-Attacks.html",
    "title": "27  Denial-of-Service Attacks",
    "section": "",
    "text": "Denial-of-Service (DoS) and Distributed Denial-of-Service (DDoS) attacks are significant threats to web applications and infrastructure. These attacks aim to make your website or application unavailable to users by overwhelming it with excessive network traffic. Let’s break down these concepts and explore how AWS can help protect against them.\n\n27.0.1 What is a Denial-of-Service (DoS) Attack?\nA Denial-of-Service (DoS) attack is a deliberate attempt to make a website or application unavailable to users. This is done by flooding the target with excessive network traffic until it becomes overloaded and can no longer respond to legitimate requests. Imagine a prankster calling a coffee shop repeatedly to place fake orders, preventing real customers from getting through. This disruption denies service to actual customers.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n27.0.2 What is a Distributed Denial-of-Service (DDoS) Attack?\nA Distributed Denial-of-Service (DDoS) attack is a more complex version of a DoS attack. In a DDoS attack, multiple sources are used to initiate the attack, making it harder to block. These sources can include multiple infected computers, known as “bots,” that send excessive traffic to a website or application simultaneously. This is akin to the prankster enlisting friends to call the coffee shop from different phone numbers, making it nearly impossible to block all the calls.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n27.0.3 Examples of DDoS Attacks\n\nUDP Flood: This attack leverages the User Datagram Protocol (UDP) by sending a simple request to a server, like the National Weather Service, but with a fake return address (your server’s address). The server then floods your server with massive amounts of data, overwhelming it.\nHTTP Level Attacks: These attacks mimic normal user behaviour, such as repeated, complicated product searches, coming from an army of zombified bot machines. The sheer volume of requests prevents regular customers from accessing the service.\nSlowloris Attack: This sophisticated attack involves sending partial HTTP requests. The server holds connections open, waiting for the complete request, thus exhausting its capacity to handle new connections.\n\n\n\n27.0.4 AWS Solutions for Mitigating DDoS Attacks\nAWS offers several tools and services to protect against DoS and DDoS attacks:\n\nSecurity Groups: These act as virtual firewalls to control inbound and outbound traffic to your instances. They help block unwanted traffic at the network level, making it difficult for low-level network attacks like UDP floods to succeed.\nElastic Load Balancer (ELB): The ELB distributes incoming application traffic across multiple targets, such as EC2 instances. It handles HTTP traffic and waits until a message is complete before passing it to your server. ELB is scalable and operates at the AWS region level, making it challenging for attackers to overwhelm it.\nAWS Shield: This is a managed DDoS protection service with two levels:\n\nAWS Shield Standard: Automatically protects all AWS customers at no cost from the most common DDoS attacks. It uses various analysis techniques to detect and mitigate malicious traffic in real time.\nAWS Shield Advanced: A paid service that provides enhanced protection, detailed attack diagnostics, and the ability to detect and mitigate sophisticated DDoS attacks. It integrates with services like Amazon CloudFront, Amazon Route 53, and Elastic Load Balancing, and allows custom rules via AWS WAF to tackle complex DDoS threats.\n\n\n\n\n27.0.5 Conclusion\nDenial-of-Service and Distributed Denial-of-Service attacks pose serious threats to the availability of your applications. Understanding these attacks and leveraging AWS tools like Security Groups, Elastic Load Balancer, and AWS Shield can significantly enhance your defence mechanisms. By using these AWS services, you can better protect your applications from disruptions, ensuring continuous and reliable service for your users.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>**Denial-of-Service Attacks**</span>"
    ]
  },
  {
    "objectID": "module-6-notes/7-Additional-Security.html",
    "href": "module-6-notes/7-Additional-Security.html",
    "title": "28  Additional Security Services in AWS",
    "section": "",
    "text": "28.0.1 The Concept of Encryption\nEncryption is a method of securing messages or data so that only authorised parties can access them. Unauthorised parties are therefore unlikely or unable to access the data. Imagine it as a key and door scenario: if you have the key, you can unlock the door; if you don’t, you cannot.\n\n\n28.0.2 Encryption in AWS\nAt AWS, encryption comes in two variations: encryption at rest and encryption in transit.\n\nEncryption at Rest: This refers to data being secured while it is stored and not actively moving. For example, server-side encryption at rest is enabled on all DynamoDB table data, helping prevent unauthorised access. DynamoDB’s encryption at rest integrates with AWS Key Management Service (KMS) for managing the encryption key used to encrypt your tables. Without this key, you cannot access your data, so it is vital to keep it safe.\nEncryption in Transit: This means data is secured while it is moving between two points, such as between an AWS service and a client. For example, when connecting a Redshift instance to a SQL client, secure sockets layer (SSL) connections are used to encrypt data, and service certificates validate and authorise the client. This ensures data is protected while passing between Redshift and the client. Similar functionality exists in numerous other AWS services, such as SQS, S3, and RDS.\n\n\n\n28.0.3 Amazon Inspector\nAmazon Inspector helps improve the security and compliance of your AWS-deployed applications by running automated security assessments against your infrastructure. It checks for deviations from security best practices, exposure of EC2 instances, vulnerabilities, and more. The service consists of three parts:\n\nNetwork Configuration Reachability: Examines the network configurations.\nAmazon Agent: Installed on EC2 instances to gather data.\nSecurity Assessment Service: Analyses data and provides findings.\n\nTo use Amazon Inspector, configure the options, run the service, and it will generate a list of potential security issues. These findings are displayed in the Amazon Inspector console with detailed descriptions and recommendations for remediation. You can also retrieve findings through an API.\n\n\n28.0.4 Amazon GuardDuty\nAmazon GuardDuty is a threat detection service that continuously analyses metadata from your account and network activity. It utilises AWS CloudTrail events, Amazon VPC Flow Logs, and DNS logs. GuardDuty uses integrated threat intelligence, anomaly detection, and machine learning to identify threats accurately. It runs independently from your other AWS services, ensuring that it does not affect their performance or availability.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n28.0.5 AWS Key Management Service (AWS KMS)\nAWS Key Management Service (KMS) enables you to perform encryption operations using cryptographic keys, which are random strings of digits used for encrypting and decrypting data. AWS KMS allows you to create, manage, and use cryptographic keys and control their use across various services and applications. You can specify which IAM users and roles can manage keys and even temporarily disable keys to prevent their use. Your keys never leave AWS KMS, ensuring you maintain control.\n\n\n28.0.6 AWS Web Application Firewall (WAF)\nAWS WAF is a web application firewall that monitors network requests to your web applications. It works with Amazon CloudFront and an Application Load Balancer to block or allow traffic using a web access control list (ACL).\n\n\n28.0.7 Example of AWS WAF in Action\nSuppose your application is receiving malicious network requests from several IP addresses. You want to prevent these requests but allow legitimate users to access your application. You configure the web ACL to allow all requests except those from the specified IP addresses. When a request comes into AWS WAF, it checks against the rules in the web ACL. If the request does not come from a blocked IP address, it allows access to the application.\n\n\nSource: AWS Cloud Practitioner Essentials\n\nIf a request comes from one of the blocked IP addresses specified in the web ACL, AWS WAF denies access.\n\n\nSource: AWS Cloud Practitioner Essentials\n\n\n\n28.0.8 Conclusion\nAWS offers a robust set of security services to help protect your applications and data. From encryption and automated security assessments with Amazon Inspector to intelligent threat detection with Amazon GuardDuty, AWS provides tools to enhance your security posture. Understanding and utilising these services ensures that your applications are secure, compliant, and resilient against various threats.",
    "crumbs": [
      "Module 6",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Additional Security Services in AWS</span>"
    ]
  },
  {
    "objectID": "module-1-notes/Introduction.html",
    "href": "module-1-notes/Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What is a Server?\nA server is a powerful computer that stores and sends data across a network. It receives requests from clients (other computers), processes these requests, and sends back the needed responses. This allows us to use services like web hosting, email, and cloud storage.\n\n\n\nThe Client-Server Model\nImagine the client-server model as a restaurant. You, the customer, ask for something (like ordering food). The restaurant (server) takes your order, prepares your food (processes your request), and brings you your meal (sends the response). In computing, a client is any device that asks for services or information, like a web browser or an app. The server is like Amazon Elastic Compute Cloud (Amazon EC2), which handles these requests and responds.\n\nFor example, a client might be looking for a news article, the latest game score, or a video. The server checks the request and responds by sending the information back to the client.\n\n\n\nLesson Recap\nToday, we learned about servers and the client-server model. Servers are the powerful computers that manage requests and send data over networks. The client-server model helps us understand how our devices communicate with servers to fetch information or use services. This basic concept is important for anyone starting to learn about how networks and the internet work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "module-2-notes/Module-2-Amazon-EC2.html",
    "href": "module-2-notes/Module-2-Amazon-EC2.html",
    "title": "Module 2",
    "section": "",
    "text": "Amazon EC2",
    "crumbs": [
      "Module 2"
    ]
  },
  {
    "objectID": "module-2-notes/Module-2-Amazon-EC2.html#introduction",
    "href": "module-2-notes/Module-2-Amazon-EC2.html#introduction",
    "title": "Module 2",
    "section": "Introduction",
    "text": "Introduction\nImagine you run a small business that develops mobile games. As your games gain popularity, you face increasing pressure to maintain performance during peak usage times, such as when launching a new game or running a global event within the game. Initially, you hosted the games on a server in your office, but as the number of players grew, the server could not handle the load and would often crash during these critical times, leading to frustrated customers and lost revenue.\nThis is where Amazon EC2 comes into play. By using Amazon EC2, you can easily launch virtual servers and scale capacity up or down automatically, depending on the demand from your game players. This ensures that your gaming servers are stable during high traffic periods and cost-efficient during quieter times.",
    "crumbs": [
      "Module 2"
    ]
  },
  {
    "objectID": "module-2-notes/Module-2-Amazon-EC2.html#what-is-amazon-ec2",
    "href": "module-2-notes/Module-2-Amazon-EC2.html#what-is-amazon-ec2",
    "title": "Module 2",
    "section": "What is Amazon EC2?",
    "text": "What is Amazon EC2?\nAmazon EC2 (Elastic Compute Cloud) provides scalable computing capacity in the AWS cloud. This service allows you to launch virtual servers, known as instances, and manage the computing environment’s scale and administration. With EC2, you can choose from a wide range of instance types to match your specific workload needs, from a small game server to a large, resource-intensive application. The service offers flexibility in configuring hardware, security, and networking settings. Additionally, it integrates seamlessly with other AWS services, enhancing your ability to develop, monitor, and deploy applications more efficiently.\nWhether it’s ensuring that your mobile games perform flawlessly during peak usage or scaling down to save costs when demand is lower, Amazon EC2 provides the tools and flexibility to adjust your computing resources in real-time, aligning perfectly with your business needs.\nKey Features of Amazon EC2:\n\nFlexibility: You can choose from a wide variety of instance types, configurations, and sizes, which allows you to tailor the hardware to your specific application needs. This includes configurations that optimize for memory, CPU, storage, and networking capacity.\nScalability: EC2 provides the ability to scale up or down quickly to handle changes in requirements or spikes in popularity, ensuring you only pay for what you use.\nControl: You have complete control over your virtual servers, including the choice of operating system, networking details, and security settings. This makes it possible to run any software you own, just as you would on your own physical server.\nIntegration: EC2 integrates well with other AWS services, facilitating comprehensive cloud solutions that can include storage (Amazon S3), databases (Amazon RDS), and more.\nSecurity: Amazon EC2 provides numerous security tools and features, such as Amazon VPC (Virtual Private Cloud) that allows you to use isolated networks within the cloud, and IAM (Identity and Access Management) to control access to instances securely.\n\nTypical Uses for Amazon EC2:\n\nWeb hosting: Many businesses use EC2 instances to host websites, ensuring they can easily handle unexpected traffic spikes.\nApplication hosting: From simple applications to sophisticated enterprise applications, you can run them all on EC2.\nBatch processing: You can quickly scale up EC2 instances to complete batch processing jobs that require processing large volumes of data quickly.\nDevelopment and test environments: Developers use EC2 to quickly set up or tear down environments with different configurations to test new versions of applications.\n\nAmazon EC2 provides a flexible, scalable, and efficient way to run your applications in the cloud with minimal investment in physical hardware and allows for a pay-as-you-go pricing model, which can significantly reduce your IT costs and overhead.",
    "crumbs": [
      "Module 2"
    ]
  },
  {
    "objectID": "module-2-notes/Module-2-Amazon-EC2.html#how-amazon-ec2-works",
    "href": "module-2-notes/Module-2-Amazon-EC2.html#how-amazon-ec2-works",
    "title": "Module 2",
    "section": "How Amazon EC2 works",
    "text": "How Amazon EC2 works\nAmazon EC2 (Elastic Compute Cloud) is a central part of Amazon Web Services that offers scalable computing on demand, allowing users to run and manage server instances over the cloud. Here’s a simplified breakdown of how you typically interact with Amazon EC2, from launch to connection and usage:\n\nLaunch\nTo begin using Amazon EC2, you start by launching a virtual server, known as an instance. Here’s how that works:\n\nChoose an AMI (Amazon Machine Image): This is your first step. An AMI contains the operating system and the configurations required to launch your instance. You can choose from a variety of AMIs that Amazon provides or create your own.\nSelect an Instance Type: Amazon EC2 offers a range of instance types optimized for different purposes. Depending on your needs, you might select an instance with more CPU, memory, storage, or enhanced networking capabilities.\nConfigure Instance: Set up the networking and security for your instance. This includes choosing a network (VPC), subnets, and setting security groups which dictate the ports, protocols, and IPs allowed to interact with your instances.\nAdd Storage: EC2 allows you to attach storage to your instances. You can choose the type and size of storage based on your application needs.\nLaunch Instance: Once everything is set up, you launch the instance. AWS then allocates the resources and starts the instance after which it’s ready to use.\n\n\n\nConnect\nOnce your EC2 instance is running, you can connect to it:\n\nAccessing the Instance:\n\nFor Linux instances, you typically connect via SSH using a key pair that you specify when setting up the instance. This ensures secure access without needing a password.\nFor Windows instances, you can connect using Remote Desktop Protocol (RDP) with a username and password, which you can retrieve using your key pair.\n\n\nThis step is crucial as it’s where you manage the software side of your instance, installing necessary applications and configuring settings according to your project’s requirements.\n\n\nUse\nAfter connecting to your instance, you can use it just like any other computer. Here’s what generally happens in this phase:\n\nRun Applications: You can deploy and run applications, host websites, and manage data. Whatever tasks you would do on a physical server can be done on an EC2 instance.\nMonitor and Manage: AWS provides tools like Amazon CloudWatch to monitor the performance of your instance. You can track metrics such as CPU utilization, and network usage, and set up alarms for specific thresholds.\nScale: One of the significant advantages of EC2 is its scalability. Depending on the demand, you can scale your instances up or down. You can either do this manually or set up auto-scaling to adjust the capacity based on pre-defined rules and schedules.\nSecure: Continuously manage the security of your instances by updating security groups, adding rules, and ensuring your software is up-to-date with the latest security patches.\n\nBy the end of this process, you will have a fully functional virtual server ready to handle your computing tasks in the cloud, providing the flexibility to scale and adapt as your requirements evolve. This capability makes EC2 a powerful tool for businesses needing reliable, scalable, and efficient computing resources.",
    "crumbs": [
      "Module 2"
    ]
  },
  {
    "objectID": "module-3-notes/Introduction-module-3.html",
    "href": "module-3-notes/Introduction-module-3.html",
    "title": "Module 3",
    "section": "",
    "text": "AWS Global Infrastructure\nThe AWS Global Infrastructure is the backbone of AWS cloud services, providing a robust, secure, and scalable platform for the deployment of applications and services. It’s not just a collection of data centres, but a network of interconnected facilities spread across the globe, known as Regions.\nWhy is this important, you ask? Imagine you’re running a global e-commerce business. Your customers are not just in one location, but scattered around the world. With AWS Global Infrastructure, you can ensure that your website is always available and delivers fast performance to all your customers, no matter where they are.\nThis is achieved through a combination of multiple Availability Zones within each Region, and edge locations used by Amazon CloudFront, AWS’s content delivery network service. This design not only ensures high availability and fault tolerance but also improves user experience by reducing latency.\nIn the upcoming lessons, we will delve deeper into these topics:\n\nWe’ll summarise the benefits of the AWS Global Infrastructure, exploring how it supports scalability, reliability, security, and global reach.\nWe’ll describe the basic concept of Availability Zones, which are essentially isolated locations within a Region to run your applications and databases.\nWe’ll discuss the benefits of Amazon CloudFront and edge locations, and how they help deliver content to your users with low latency and high transfer speeds.\nLastly, we’ll compare different methods for provisioning AWS services, helping you understand the various ways you can create and manage your AWS resources.",
    "crumbs": [
      "Module 3"
    ]
  },
  {
    "objectID": "module-4-notes/1-Introduction-m4.html",
    "href": "module-4-notes/1-Introduction-m4.html",
    "title": "Module 4",
    "section": "",
    "text": "Introduction to AWS Networking and Security\nIn the age of cloud computing, understanding networking and security is critical. As more and more of our workloads migrate to the cloud, the demand for secure, dependable, and efficient networking has never been higher. This is where Amazon Web Services, a renowned cloud service provider, comes into action.\nAWS offers a wide range of services and products for developing and deploying large-scale applications. However, in order to take full advantage of these tools, you must first grasp how AWS networking and security function. This knowledge will enable you to create and deploy powerful, scalable, and secure apps on AWS.\nThis module is meant to provide you with this crucial knowledge. It serves as the foundation for your journey towards AWS Cloud Practitioner certification.\nHere are the learning objectives for this module:\n\nDescribe the basic concepts of networking.\nDescribe the difference between public and private networking resources.\nExplain a virtual private gateway using a real-life scenario.\nExplain a virtual private network (VPN) using a real-life scenario.\nDescribe the benefit of AWS Direct Connect.\nDescribe the benefit of hybrid deployments.\nDescribe the layers of security used in an IT strategy.\nDescribe the services customers use to interact with the AWS global network.",
    "crumbs": [
      "Module 4"
    ]
  },
  {
    "objectID": "module-5-notes/1-Introduction-m5.html",
    "href": "module-5-notes/1-Introduction-m5.html",
    "title": "Module 5",
    "section": "",
    "text": "Introduction\n\nStorage and databases on AWS\nBased on what we’ve discussed so far, it is possible to build an elastic, scalable, disaster-resistant, and cost-optimized system via AWS, with a worldwide, highly secure network that can be installed totally programmatically.\nSome cloud-based activities may require data storage, such as client information and loyalty programs. That means we need to store info and use databases. Not just any database will do; we must be sure to select the appropriate database for the job and the appropriate storage for the various data kinds.\nYou may have many different data usage needs and data types. In this module, we will learn about the different services AWS offers to help you build the perfect data solution.\nYou will learn to:\n\nSummarise the basic concepts of storage and databases.\nDescribe the benefits of the Amazon Elastic Block Store (Amazon EBS).\nDescribe the benefits of Amazon Simple Storage Service (Amazon S3).\nDescribe the benefits of the Amazon Elastic File System (Amazon EFS).\nSummarise various storage solutions.\nDescribe the benefits of the Amazon Relational Database Service (Amazon RDS).\nDescribe the benefits of Amazon DynamoDB.\nSummarise various database services.",
    "crumbs": [
      "Module 5"
    ]
  }
]